<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift | Amy on GCP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift" />
<meta property="og:description" content="how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift" />
<link rel="canonical" href="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html" />
<meta property="og:url" content="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html" />
<meta property="og:site_name" content="Amy on GCP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-26T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2021-02-26T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html"},"description":"how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift","@type":"BlogPosting","url":"https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html","headline":"Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift","dateModified":"2021-02-26T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/gcp_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://amygdala.github.io/gcp_blog/feed.xml" title="Amy on GCP" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164080601-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/gcp_blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift | Amy on GCP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift" />
<meta property="og:description" content="how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift" />
<link rel="canonical" href="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html" />
<meta property="og:url" content="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html" />
<meta property="og:site_name" content="Amy on GCP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-26T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2021-02-26T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html"},"description":"how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift","@type":"BlogPosting","url":"https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html","headline":"Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift","dateModified":"2021-02-26T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://amygdala.github.io/gcp_blog/feed.xml" title="Amy on GCP" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164080601-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/gcp_blog/">Amy on GCP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gcp_blog/about/">About Me</a><a class="page-link" href="/gcp_blog/search/">Search</a><a class="page-link" href="/gcp_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift</h1><p class="page-description">how to set up event-triggered Kubeflow Pipelines runs, and use TFDV to detect data drift</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-26T00:00:00-06:00" itemprop="datePublished">
        Feb 26, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/gcp_blog/categories/#ml">ml</a>
        &nbsp;
      
        <a class="category-tags-link" href="/gcp_blog/categories/#kfp">kfp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/gcp_blog/categories/#mlops">mlops</a>
        &nbsp;
      
        <a class="category-tags-link" href="/gcp_blog/categories/#tfdv">tfdv</a>
        &nbsp;
      
        <a class="category-tags-link" href="/gcp_blog/categories/#gcf">gcf</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#running-the-example-notebook">Running the example notebook</a></li>
<li class="toc-entry toc-h2"><a href="#creating-tfdv-based-kfp-components">Creating TFDV-based KFP components</a></li>
<li class="toc-entry toc-h2"><a href="#defining-a-pipeline-that-uses-the-tfdv-components">Defining a pipeline that uses the TFDV components</a>
<ul>
<li class="toc-entry toc-h3"><a href="#instantiate-pipeline-ops-from-the-components">Instantiate pipeline ops from the components</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#event-triggered-pipeline-runs">Event-triggered pipeline runs</a>
<ul>
<li class="toc-entry toc-h3"><a href="#set-up-a-gcf-function-to-trigger-a-pipeline-run-when-a-dataset-is-updated">Set up a GCF function to trigger a pipeline run when a dataset is updated</a>
<ul>
<li class="toc-entry toc-h4"><a href="#define-the-gcf-function">Define the GCF function</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#trigger-a-pipeline-run-when-new-data-becomes-available">Trigger a pipeline run when new data becomes available</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#summary">Summary</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>With ML workflows, it is often insufficient to train and deploy a given model just once.  Even if the model has desired accuracy initially, this can change if the data used for making prediction requests becomesâ€” perhaps over timeâ€” sufficiently different from the data used to originally train the model.</p>

<p>When new data becomes available, which could be used for retraining a model, it can be helpful to apply techniques for analyzing <em>data â€˜driftâ€™</em>, and determining whether the drift is sufficiently anomalous to warrant retraining yet.
It can also be useful to trigger such an analysisâ€” and potential re-run of your training pipelineâ€” <em>automatically</em>, upon arrival of new data.</p>

<p>This blog post highlights an <a href="https://github.com/amygdala/code-snippets/blob/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">example notebook</a> that shows how to set up such a scenario with <a href="https://www.kubeflow.org/docs/pipelines/">Kubeflow Pipelines</a> (KFP).
It shows how to build a pipeline that checks for statistical drift across successive versions of a dataset and uses that information to make a decision on whether to (re)train a model<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>; and how to configure event-driven deployment of pipeline jobs when new data arrives.</p>

<p>The notebook builds on an example highlighted in a <a href="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html">previous blog post</a> â€” which shows a KFP training and serving pipelineâ€” and introduces two primary new concepts:</p>

<ul>
  <li>the example demonstrates use of the <a href="https://www.tensorflow.org/tfx/guide/tfdv">TensorFlow Data Validation (TFDV)</a> library to build pipeline components that derive <strong>dataset statistics</strong> and detect <strong>drift</strong> between older and newer dataset versions, and shows how to use drift information to decide whether to retrain a model on newer data.</li>
  <li>the example shows how to support <strong>event-triggered</strong> launch of Kubeflow Pipelines runs from a <a href="https://cloud.google.com/functions/docs/">Cloud Functions</a> (GCF) function, where the Function run is triggered by addition of a file to a given <a href="https://cloud.google.com/storage">Cloud Storage</a> (GCS) bucket.</li>
</ul>

<p>The machine learning task uses a tabular dataset that joins London bike rental information with weather data, and train a Keras model to predict rental duration. See <a href="https://amygdala.github.io/gcp_blog/ml/kfp/kubeflow/keras/tensorflow/hp_tuning/2020/10/19/keras_tuner.html">this</a> and <a href="https://amygdala.github.io/gcp_blog/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html">this</a> blog post and associated <a href="https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md">README</a> for more background on the dataset and model architecture.</p>

<figure>
<a href="https://storage.googleapis.com/amy-jo/images/kf-pls/CleanShot%202021-02-26%20at%2011.30.13%402x.png" target="_blank"><img src="https://storage.googleapis.com/amy-jo/images/kf-pls/CleanShot%202021-02-26%20at%2011.30.13%402x.png" width="80%"></a>
<figcaption><br><i>A pipeline run using TFDV-based components to detect 'data drift'.</i></figcaption>
</figure>

<h2 id="running-the-example-notebook">
<a class="anchor" href="#running-the-example-notebook" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running the example notebook</h2>

<p>The <a href="https://github.com/amygdala/code-snippets/blob/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">example notebook</a> requires a <a href="https://cloud.google.com/">Google Cloud Platform (GCP)</a> account and project, ideally with quota for using GPUs, andâ€” as detailed in the notebookâ€” an installation of <strong>AI Platform Pipelines (Hosted Kubeflow Pipelines)</strong> (that is, an installation of KFP on <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine (GKE)</a>), with a few additional configurations once installation is complete.</p>

<p>The notebook can be run using either <a href="https://colab.research.google.com/">Colab</a> (<a href="https://colab.research.google.com/github/amygdala/code-snippets/blob/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">open directly</a>) or <a href="https://cloud.google.com/ai-platform-notebooks">AI Platform Notebooks</a> (<a href="https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">open directly</a>).</p>

<h2 id="creating-tfdv-based-kfp-components">
<a class="anchor" href="#creating-tfdv-based-kfp-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating TFDV-based KFP components</h2>

<p>Our first step is to build the TFDV components that we want to use in our pipeline.</p>

<blockquote>
  <p>Note: For this example, our training data is in GCS, in CSV-formatted files.  So, we can take advantage of TFDVâ€™s ability to process CSV files.  The TFDV libraries can also process files in <code class="highlighter-rouge">TFRecords</code> format.</p>
</blockquote>

<p>Weâ€™ll define both TFDV KFP pipeline <em>components</em> as <a href="https://www.kubeflow.org/docs/pipelines/sdk/python-function-components/">â€˜lightweightâ€™ Python-function-based components</a>. For each component, we define a function, then call <code class="highlighter-rouge">kfp.components.func_to_container_op()</code> on that function to build a <strong>reusable</strong> component in <code class="highlighter-rouge">.yaml</code> format.
Letâ€™s take a closer look at how this works (details are in the <a href="https://github.com/amygdala/code-snippets/blob/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">notebook</a>).</p>

<p>Below is the Python function weâ€™ll use to generate TFDV statistics from a collection of <code class="highlighter-rouge">csv</code> files.  The functionâ€” and the component weâ€™ll create from itâ€” outputs the path to the generated stats file.  When we define a pipeline that uses this component, weâ€™ll use this stepâ€™s output as input to another pipeline step.
TFDV uses a <a href="https://beam.apache.org/">Beam</a> pipelineâ€” not to be confused with KFP Pipelinesâ€” to implement the stats generation. Depending upon configuration, the component can use either the Direct (local) runner or the <a href="https://cloud.google.com/dataflow#section-5">Dataflow</a> runner.  Running the Beam pipeline on Dataflow rather than locally can make sense with large datasets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span>

<span class="k">def</span> <span class="nf">generate_tfdv_stats</span><span class="p">(</span><span class="n">input_data</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">job_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">use_dataflow</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                        <span class="n">project_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">region</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">gcs_temp_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">gcs_staging_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                        <span class="n">whl_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">''</span><span class="p">,</span> <span class="n">requirements_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'requirements.txt'</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NamedTuple</span><span class="p">(</span><span class="s">'Outputs'</span><span class="p">,</span> <span class="p">[(</span><span class="s">'stats_path'</span><span class="p">,</span> <span class="nb">str</span><span class="p">)]):</span>

  <span class="kn">import</span> <span class="nn">logging</span>
  <span class="kn">import</span> <span class="nn">time</span>

  <span class="kn">import</span> <span class="nn">tensorflow_data_validation</span> <span class="k">as</span> <span class="n">tfdv</span>
  <span class="kn">import</span> <span class="nn">tensorflow_data_validation.statistics.stats_impl</span>
  <span class="kn">from</span> <span class="nn">apache_beam.options.pipeline_options</span> <span class="kn">import</span> <span class="n">PipelineOptions</span><span class="p">,</span> <span class="n">GoogleCloudOptions</span><span class="p">,</span> <span class="n">StandardOptions</span><span class="p">,</span> <span class="n">SetupOptions</span>

  <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">"output path: </span><span class="si">%</span><span class="s">s"</span><span class="p">,</span> <span class="n">output_path</span><span class="p">)</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Building pipeline options"</span><span class="p">)</span>
  <span class="c1"># Create and set your PipelineOptions.
</span>  <span class="n">options</span> <span class="o">=</span> <span class="n">PipelineOptions</span><span class="p">()</span>

  <span class="k">if</span> <span class="n">use_dataflow</span> <span class="o">==</span> <span class="s">'true'</span><span class="p">:</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">"using Dataflow"</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">whl_location</span><span class="p">:</span>
      <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s">'tfdv whl file required with dataflow runner.'</span><span class="p">)</span>
      <span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">google_cloud_options</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">GoogleCloudOptions</span><span class="p">)</span>
    <span class="n">google_cloud_options</span><span class="o">.</span><span class="n">project</span> <span class="o">=</span> <span class="n">project_id</span>
    <span class="n">google_cloud_options</span><span class="o">.</span><span class="n">job_name</span> <span class="o">=</span> <span class="s">'{}-{}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">job_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())))</span>
    <span class="n">google_cloud_options</span><span class="o">.</span><span class="n">staging_location</span> <span class="o">=</span> <span class="n">gcs_staging_location</span>
    <span class="n">google_cloud_options</span><span class="o">.</span><span class="n">temp_location</span> <span class="o">=</span> <span class="n">gcs_temp_location</span>
    <span class="n">google_cloud_options</span><span class="o">.</span><span class="n">region</span> <span class="o">=</span> <span class="n">region</span>
    <span class="n">options</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">StandardOptions</span><span class="p">)</span><span class="o">.</span><span class="n">runner</span> <span class="o">=</span> <span class="s">'DataflowRunner'</span>

    <span class="n">setup_options</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">SetupOptions</span><span class="p">)</span>
    <span class="n">setup_options</span><span class="o">.</span><span class="n">extra_packages</span> <span class="o">=</span> <span class="p">[</span><span class="n">whl_location</span><span class="p">]</span>
    <span class="n">setup_options</span><span class="o">.</span><span class="n">requirements_file</span> <span class="o">=</span> <span class="s">'requirements.txt'</span>

  <span class="n">tfdv</span><span class="o">.</span><span class="n">generate_statistics_from_csv</span><span class="p">(</span>
    <span class="n">data_location</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">output_path</span><span class="p">,</span>
    <span class="n">pipeline_options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>

  <span class="k">return</span> <span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="p">)</span>
</code></pre></div></div>

<p>To turn this function into a KFP <em>component</em>, weâ€™ll call <code class="highlighter-rouge">kfp.components.func_to_container_op()</code>.  Weâ€™re passing it a base container image to use: <code class="highlighter-rouge">gcr.io/google-samples/tfdv-tests:v1</code>.  This base image has the TFDV libraries already installed, so that we donâ€™t need to install them â€˜inlineâ€™ when we run a pipeline step based on this component.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">kfp</span>
<span class="n">kfp</span><span class="o">.</span><span class="n">components</span><span class="o">.</span><span class="n">func_to_container_op</span><span class="p">(</span><span class="n">generate_tfdv_stats</span><span class="p">,</span>
    <span class="n">output_component_file</span><span class="o">=</span><span class="s">'tfdv_component.yaml'</span><span class="p">,</span> <span class="n">base_image</span><span class="o">=</span><span class="s">'gcr.io/google-samples/tfdv-tests:v1'</span><span class="p">)</span>
</code></pre></div></div>

<p>Weâ€™ll take the same approach to build a second TFDV-based component, one which detects <em>drift</em> between datasets by comparing their stats.  The TFDV library makes this straightforward.  Weâ€™re using a drift comparator appropriate for a <em>regression</em> modelâ€” as used in the example pipelineâ€” and looking for drift on a given set of fields (in this case, for example purposes, just one).
The <code class="highlighter-rouge">tensorflow_data_validation.validate_statistics()</code> call will then tell us whether the drift anomaly for that field is over the specified threshold. See the <a href="https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift">TFDV docs</a> for more detail.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">schema1</span> <span class="o">=</span> <span class="n">tfdv</span><span class="o">.</span><span class="n">infer_schema</span><span class="p">(</span><span class="n">statistics</span><span class="o">=</span><span class="n">stats1</span><span class="p">)</span>
  <span class="n">tfdv</span><span class="o">.</span><span class="n">get_feature</span><span class="p">(</span><span class="n">schema1</span><span class="p">,</span> <span class="s">'duration'</span><span class="p">)</span><span class="o">.</span><span class="n">drift_comparator</span><span class="o">.</span><span class="n">jensen_shannon_divergence</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.01</span>
  <span class="n">drift_anomalies</span> <span class="o">=</span> <span class="n">tfdv</span><span class="o">.</span><span class="n">validate_statistics</span><span class="p">(</span>
      <span class="n">statistics</span><span class="o">=</span><span class="n">stats2</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema1</span><span class="p">,</span> <span class="n">previous_statistics</span><span class="o">=</span><span class="n">stats1</span><span class="p">)</span>
</code></pre></div></div>

<p>(The details of this second component definition are in the example notebook).</p>

<h2 id="defining-a-pipeline-that-uses-the-tfdv-components">
<a class="anchor" href="#defining-a-pipeline-that-uses-the-tfdv-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Defining a pipeline that uses the TFDV components</h2>

<p>After weâ€™ve defined both TFDV componentsâ€” one to generate stats for a dataset, and one to detect drift between datasetsâ€” weâ€™re ready to build a Kubeflow Pipeline that uses these components, in conjunction with previously-built components for a training &amp; serving workflow.</p>

<h3 id="instantiate-pipeline-ops-from-the-components">
<a class="anchor" href="#instantiate-pipeline-ops-from-the-components" aria-hidden="true"><span class="octicon octicon-link"></span></a>Instantiate pipeline <em>ops</em> from the components</h3>

<p>KFP components in <code class="highlighter-rouge">yaml</code> format are shareable and reusable.  Weâ€™ll build our pipeline by starting with some already-built componentsâ€” (described in more detail <a href="https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md">here</a>)â€” that support our basic â€˜train/evaluate/deployâ€™ workflow.</p>

<p>Weâ€™ll instantiate some pipeline ops from these pre-existing components like this, by loading them via URL:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">kfp.components</span> <span class="k">as</span> <span class="n">comp</span>

<span class="c1"># pre-existing components
</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">comp</span><span class="o">.</span><span class="n">load_component_from_url</span><span class="p">(</span>
  <span class="s">'https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/train_component.yaml'</span>
  <span class="p">)</span>
<span class="o">...</span> <span class="n">etc</span><span class="o">.</span> <span class="o">...</span>
</code></pre></div></div>

<p>â€¦ then create our TFDV ops from the new components we just built:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">tfdv_op</span> <span class="o">=</span> <span class="n">comp</span><span class="o">.</span><span class="n">load_component_from_file</span><span class="p">(</span>
  <span class="s">'tfdv_component.yaml'</span>
  <span class="p">)</span>
<span class="n">tfdv_drift_op</span> <span class="o">=</span> <span class="n">comp</span><span class="o">.</span><span class="n">load_component_from_file</span><span class="p">(</span>
  <span class="s">'tfdv_drift_component.yaml'</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>Then, we define a KFP pipeline from the defined ops.  Weâ€™re not showing the pipeline in full hereâ€” see the notebook for details.
Two pipeline steps use the <code class="highlighter-rouge">tfdv_op</code>, which generates the stats.  <code class="highlighter-rouge">tfdv1</code> generates stats for the test data, and <code class="highlighter-rouge">tfdv2</code> for the training data.
In the following, you can see that the <code class="highlighter-rouge">tfdv_drift</code> step takes as input the output from the <code class="highlighter-rouge">tfdv2</code> (stats for training data) step.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dsl</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
  <span class="n">name</span><span class="o">=</span><span class="s">'bikes_weather_tfdv'</span><span class="p">,</span>
  <span class="n">description</span><span class="o">=</span><span class="s">'Model bike rental duration given weather'</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">bikes_weather_tfdv</span><span class="p">(</span>
  <span class="o">...</span> <span class="n">other</span> <span class="n">pipeline</span> <span class="n">params</span> <span class="o">...</span>
  <span class="n">working_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'gs://YOUR/GCS/PATH'</span><span class="p">,</span>
  <span class="n">data_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'gs://aju-dev-demos-codelabs/bikes_weather/'</span><span class="p">,</span>
  <span class="n">project_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'YOUR-PROJECT-ID'</span><span class="p">,</span>
  <span class="n">region</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'us-central1'</span><span class="p">,</span>
  <span class="n">requirements_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'requirements.txt'</span><span class="p">,</span>
  <span class="n">job_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'test'</span><span class="p">,</span>
  <span class="n">whl_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'tensorflow_data_validation-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl'</span><span class="p">,</span>
  <span class="n">use_dataflow</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">''</span><span class="p">,</span>
  <span class="n">stats_older_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb'</span>
  <span class="p">):</span>
  <span class="o">...</span>

  <span class="n">tfdv1</span> <span class="o">=</span> <span class="n">tfdv_op</span><span class="p">(</span>  <span class="c1"># TFDV stats for the test data
</span>    <span class="n">input_data</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">stest-*.csv'</span> <span class="o">%</span> <span class="p">(</span><span class="n">data_dir</span><span class="p">,),</span>
    <span class="n">output_path</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers/</span><span class="si">%</span><span class="s">s/eval/evaltest.pb'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,</span> <span class="n">dsl</span><span class="o">.</span><span class="n">RUN_ID_PLACEHOLDER</span><span class="p">),</span>
    <span class="n">job_name</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s-1'</span> <span class="o">%</span> <span class="p">(</span><span class="n">job_name</span><span class="p">,),</span>
    <span class="n">use_dataflow</span><span class="o">=</span><span class="n">use_dataflow</span><span class="p">,</span>
    <span class="n">project_id</span><span class="o">=</span><span class="n">project_id</span><span class="p">,</span> <span class="n">region</span><span class="o">=</span><span class="n">region</span><span class="p">,</span>
    <span class="n">gcs_temp_location</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers/tmp'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,),</span>
    <span class="n">gcs_staging_location</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,),</span>
    <span class="n">whl_location</span><span class="o">=</span><span class="n">whl_location</span><span class="p">,</span> <span class="n">requirements_file</span><span class="o">=</span><span class="n">requirements_file</span>
    <span class="p">)</span>
  <span class="n">tfdv2</span> <span class="o">=</span> <span class="n">tfdv_op</span><span class="p">(</span>  <span class="c1"># TFDV stats for the training data
</span>    <span class="n">input_data</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">strain-*.csv'</span> <span class="o">%</span> <span class="p">(</span><span class="n">data_dir</span><span class="p">,),</span>
    <span class="c1"># output_path='%s/%s/eval/evaltrain.pb' % (output_path, dsl.RUN_ID_PLACEHOLDER),
</span>    <span class="n">output_path</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers/</span><span class="si">%</span><span class="s">s/eval/evaltrain.pb'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,</span> <span class="n">dsl</span><span class="o">.</span><span class="n">RUN_ID_PLACEHOLDER</span><span class="p">),</span>
    <span class="n">job_name</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s-2'</span> <span class="o">%</span> <span class="p">(</span><span class="n">job_name</span><span class="p">,),</span>
    <span class="n">use_dataflow</span><span class="o">=</span><span class="n">use_dataflow</span><span class="p">,</span>
    <span class="n">project_id</span><span class="o">=</span><span class="n">project_id</span><span class="p">,</span> <span class="n">region</span><span class="o">=</span><span class="n">region</span><span class="p">,</span>
    <span class="n">gcs_temp_location</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers/tmp'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,),</span>
    <span class="n">gcs_staging_location</span><span class="o">=</span><span class="s">'</span><span class="si">%</span><span class="s">s/tfdv_expers'</span> <span class="o">%</span> <span class="p">(</span><span class="n">working_dir</span><span class="p">,),</span>
    <span class="n">whl_location</span><span class="o">=</span><span class="n">whl_location</span><span class="p">,</span> <span class="n">requirements_file</span><span class="o">=</span><span class="n">requirements_file</span>
    <span class="p">)</span>

  <span class="c1"># compare generated training data stats with stats from a previous version
</span>  <span class="c1"># of the training data set.
</span>  <span class="n">tfdv_drift</span> <span class="o">=</span> <span class="n">tfdv_drift_op</span><span class="p">(</span><span class="n">stats_older_path</span><span class="p">,</span> <span class="n">tfdv2</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s">'stats_path'</span><span class="p">])</span>

  <span class="c1"># proceed with training if drift is detected (or if no previous stats were provided)
</span>  <span class="k">with</span> <span class="n">dsl</span><span class="o">.</span><span class="n">Condition</span><span class="p">(</span><span class="n">tfdv_drift</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s">'drift'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'true'</span><span class="p">):</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">train_op</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">eval_metrics</span> <span class="o">=</span> <span class="n">eval_metrics_op</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">dsl</span><span class="o">.</span><span class="n">Condition</span><span class="p">(</span><span class="n">eval_metrics</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s">'deploy'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'deploy'</span><span class="p">):</span>
      <span class="n">serve</span> <span class="o">=</span> <span class="n">serve_op</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

</code></pre></div></div>

<p>While not all pipeline details are shown, you can see that this pipeline definition includes some conditional expressions; parts of the pipeline will run only if an output of an â€˜upstreamâ€™ step meets the given conditions.  We start the model training step if drift anomalies were detected.  (And, once training is completed, weâ€™ll deploy the model for serving only if its evaluation metrics meet certain thresholds).</p>

<p>Hereâ€™s the <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> for this pipeline.  You can see the conditional expressions reflected; and can see that the step to generates stats for the test dataset provides no downstream dependencies, but the stats on the training set are used as input for the drift detection step.</p>

<figure>
<a href="https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline.png" target="_blank"><img src="https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline.png" width="80%"></a>
<figcaption><br><i>The pipeline DAG</i></figcaption>
</figure>

<p>Hereâ€™s a pipeline run in progress:</p>

<figure>
<a href="https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline_run.png" target="_blank"><img src="https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline_run.png" width="80%"></a>
<figcaption><br><i>A pipeline run in progress.</i></figcaption>
</figure>

<p>See the <a href="https://github.com/amygdala/code-snippets/blob/master/ml/notebook_examples/hosted_kfp/event_triggered_kfp_pipeline_bw.ipynb">example notebook</a> for more details on how to run this pipeline.</p>

<h2 id="event-triggered-pipeline-runs">
<a class="anchor" href="#event-triggered-pipeline-runs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Event-triggered pipeline runs</h2>

<p>Once you have defined this pipeline, a next useful step is to automatically run it when an update to the dataset is available, so that each dataset update triggers an analysis of data drift and potential model (re)training.</p>

<p>Weâ€™ll show how to do this using <a href="https://cloud.google.com/functions/docs/">Cloud Functions (GCF)</a>, by setting up a function that is triggered when new data is added to a <a href="https://cloud.google.com/storage">GCS</a> bucket.</p>

<h3 id="set-up-a-gcf-function-to-trigger-a-pipeline-run-when-a-dataset-is-updated">
<a class="anchor" href="#set-up-a-gcf-function-to-trigger-a-pipeline-run-when-a-dataset-is-updated" aria-hidden="true"><span class="octicon octicon-link"></span></a>Set up a GCF function to trigger a pipeline run when a dataset is updated</h3>

<p>Weâ€™ll define and deploy a <a href="https://cloud.google.com/functions/docs/">Cloud Functions (GCF)</a> function that launches a run of this pipeline when new training data becomes available, as triggered by the creation or modification of a file in a â€˜triggerâ€™ bucket on GCS.</p>

<p>In most cases, you donâ€™t want to launch a new pipeline run for every new file added to a datasetâ€” since typically, the dataset will be comprised of a collection of files, to which you will add/update multiple files in a batch. So, you donâ€™t want the â€˜trigger bucketâ€™ to be the dataset bucket (if the data lives on GCS)â€” that will trigger unwanted pipeline runs.
Instead, weâ€™ll trigger a pipeline run after the upload of a <em>batch</em> of new data has completed.</p>

<p>To do this, weâ€™ll use an approach where the the â€˜triggerâ€™ bucket is different from the bucket used to store dataset files. â€˜Trigger filesâ€™ uploaded to that bucket are expected to contain the path of the updated dataset as well as the path to the data stats file generated for the last model trained.
A trigger file is uploaded once the new data upload has completed, and that upload triggers a run of the GCF function, which in turn reads info on the new data path from the trigger file and launches the pipeline job.</p>

<h4 id="define-the-gcf-function">
<a class="anchor" href="#define-the-gcf-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Define the GCF function</h4>

<p>To set up this process, weâ€™ll first define the GCF function in a file called <code class="highlighter-rouge">main.py</code>, as well as an accompanying requirements file in the same directory that specifies the libraries to load prior to running the function.  The requirements file will indicate to install the KFP SDK:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kfp</span><span class="o">==</span><span class="mf">1.4</span>
</code></pre></div></div>

<p>The code looks like this (with some detail removed); we parse the trigger file contents and use that information to launch a pipeline run. The code uses the values of several environment variables that we will set when uploading the GCF function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">kfp</span>
<span class="kn">from</span> <span class="nn">kfp</span> <span class="kn">import</span> <span class="n">dsl</span>
<span class="kn">from</span> <span class="nn">kfp</span> <span class="kn">import</span> <span class="n">compiler</span>
<span class="kn">from</span> <span class="nn">kfp</span> <span class="kn">import</span> <span class="n">components</span>

<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">storage</span>

<span class="n">PIPELINE_PROJECT_ID</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s">'PIPELINE_PROJECT_ID'</span><span class="p">)</span>
<span class="o">...</span><span class="n">etc</span><span class="o">...</span>

<span class="k">def</span> <span class="nf">read_trigger_file</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">storage_client</span><span class="p">):</span>
    <span class="s">"""Read the contents of the trigger file and return as string.
    """</span>
    <span class="o">....</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">storage_client</span><span class="o">.</span><span class="n">get_bucket</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'bucket'</span><span class="p">])</span>
    <span class="n">blob</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">get_blob</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'name'</span><span class="p">])</span>
    <span class="n">trigger_file_string</span> <span class="o">=</span> <span class="n">blob</span><span class="o">.</span><span class="n">download_as_string</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">'trigger file contents: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">trigger_file_string</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">trigger_file_string</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'UTF-8'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gcs_update</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="s">"""Background Cloud Function to be triggered by Cloud Storage.
    """</span>

    <span class="n">storage_client</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">Client</span><span class="p">()</span>
    <span class="c1"># get the contents of the trigger file
</span>    <span class="n">trigger_file_string</span> <span class="o">=</span> <span class="n">read_trigger_file</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">storage_client</span><span class="p">)</span>
    <span class="n">trigger_file_info</span> <span class="o">=</span> <span class="n">trigger_file_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="c1"># then run the pipeline using the given job spec, passing the trigger file contents
</span>    <span class="c1"># as parameter values.
</span>    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">'running pipeline with id </span><span class="si">%</span><span class="s">s...'</span><span class="p">,</span> <span class="n">PIPELINE_ID</span><span class="p">)</span>
    <span class="c1"># create the client object
</span>    <span class="n">client</span> <span class="o">=</span> <span class="n">kfp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="n">PIPELINE_HOST</span><span class="p">)</span>
    <span class="c1"># deploy the pipeline run
</span>    <span class="n">run</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">run_pipeline</span><span class="p">(</span><span class="n">EXP_ID</span><span class="p">,</span> <span class="s">'bw_tfdv_gcf'</span><span class="p">,</span> <span class="n">pipeline_id</span><span class="o">=</span><span class="n">PIPELINE_ID</span><span class="p">,</span>
                          <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s">'working_dir'</span><span class="p">:</span> <span class="n">WORKING_DIR</span><span class="p">,</span>
                                  <span class="s">'project_id'</span><span class="p">:</span> <span class="n">PIPELINE_PROJECT_ID</span><span class="p">,</span>
                                  <span class="s">'use_dataflow'</span><span class="p">:</span> <span class="n">USE_DATAFLOW</span><span class="p">,</span>
                                  <span class="s">'data_dir'</span><span class="p">:</span> <span class="n">trigger_file_info</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                  <span class="s">'stats_older_path'</span><span class="p">:</span> <span class="n">trigger_file_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]})</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s">'job response: </span><span class="si">%</span><span class="s">s'</span><span class="p">,</span> <span class="n">run</span><span class="p">)</span>
</code></pre></div></div>

<p>Then weâ€™ll deploy the GCF function as follows. Note that weâ€™re indicating to use the <code class="highlighter-rouge">gcs_update</code> definition (from <code class="highlighter-rouge">main.py</code>), and specifying the trigger bucket.   Note also how weâ€™re setting environment vars as part of the deployment.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud functions deploy gcs_update <span class="nt">--set-env-vars</span> <span class="se">\</span>
  <span class="nv">PIPELINE_PROJECT_ID</span><span class="o">={</span>PROJECT_ID<span class="o">}</span>,WORKING_DIR<span class="o">={</span>WORKING_DIR<span class="o">}</span>,PIPELINE_SPEC<span class="o">={</span>PIPELINE_SPEC<span class="o">}</span>,PIPELINE_ID<span class="o">={</span>PIPELINE_ID<span class="o">}</span>,PIPELINE_HOST<span class="o">={</span>PIPELINE_HOST<span class="o">}</span>,EXP_ID<span class="o">={</span>EXP_ID<span class="o">}</span>,USE_DATAFLOW<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--runtime</span> python37 <span class="nt">--trigger-resource</span> <span class="o">{</span>TRIGGER_BUCKET<span class="o">}</span> <span class="nt">--trigger-event</span> google.storage.object.finalize
</code></pre></div></div>

<h3 id="trigger-a-pipeline-run-when-new-data-becomes-available">
<a class="anchor" href="#trigger-a-pipeline-run-when-new-data-becomes-available" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trigger a pipeline run when new data becomes available</h3>

<p>Once the GCF function is set up, it will run when a file is added to (or modified in) the trigger bucket.  For this simple example, the GCF function expects trigger files of the following format, where the first line is the path to the updated dataset, and the second line is the path to the TFDV stats for the dataset used for the previously-trained model.  More generally, such a trigger file can contain whatever information is necessary to determine how to parameterize the pipeline run.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gs://path/to/new/or/updated/dataset/
gs://path/to/stats/from/previous/dataset/stats.pb
</code></pre></div></div>

<h2 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>

<p>This blog post showed how to build Kubeflow Pipeline components, using the TFDV libraries, to analyze datasets and detect data drift.  Then, it showed how to support event-triggered pipeline runs via GCF.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In this example, we show full model retraining on a new dataset.  An alternate scenarioâ€” not covered hereâ€” could involve <em>tuning</em> an existing model with new data.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">â†©</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/gcp_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/gcp_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/gcp_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Using Google Cloud Platform</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/amygdala" title="amygdala"><svg class="svg-icon grey"><use xlink:href="/gcp_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/amygdala" title="amygdala"><svg class="svg-icon grey"><use xlink:href="/gcp_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>

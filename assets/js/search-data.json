{
  
    
        "post0": {
            "title": "Creating an AutoML Tables end-to-end workflow on Cloud AI Platform Pipelines",
            "content": "AutoML Tables: end-to-end workflow on Cloud AI Platform Pipelines . Introduction . AutoML Tables lets you automatically build, analyze, and deploy state-of-the-art machine learning models using your own structured data. . A number of new AutoML Tables features have been released recently. These include: . An improved Python client library | The ability to obtain explanations for your online predictions | The ability to export your model and serve it in a container anywhere | The ability to view model search progress and final model hyperparameters in Cloud Logging | . This post gives a tour of some of these new features via a Cloud AI Platform Pipelines example, that shows end-to-end management of an AutoML Tables workflow. . The example pipeline creates a dataset, imports data into the dataset from a BigQuery view, and trains a custom model on that data. Then, it fetches evaluation and metrics information about the trained model, and based on specified criteria about model quality, uses that information to automatically determine whether to deploy the model for online prediction. Once the model is deployed, you can make prediction requests, and optionally obtain prediction explanations as well as the prediction result. In addition, the example shows how to scalably serve your exported trained model from your Cloud AI Platform Pipelines installation for prediction requests. . You can manage all the parts of this workflow from the Tables UI as well, or programmatically via a notebook or script. But specifying this process as a workflow has some advantages: the workflow becomes reliable and repeatable, and Pipelines makes it easy to monitor the results and schedule recurring runs. For example, if your dataset is updated regularly—say once a day— you could schedule a workflow to run daily, each day building a model that trains on an updated dataset. (With a bit more work, you could also set up event-based triggering pipeline runs, for example when new data is added to a Google Cloud Storage bucket.) . About the example dataset and scenario . The Cloud Public Datasets Program makes available public datasets that are useful for experimenting with machine learning. For our examples, we’ll use data that is essentially a join of two public datasets stored in BigQuery: London Bike rentals and NOAA weather data, with some additional processing to clean up outliers and derive additional GIS and day-of-week fields. Using this dataset, we’ll build a regression model to predict the duration of a bike rental based on information about the start and end rental stations, the day of the week, the weather on that day, and other data. If we were running a bike rental company, we could use these predictions—and their explanations—to help us anticipate demand and even plan how to stock each location. While we’re using bike and weather data here, you can use AutoML Tables for tasks as varied as asset valuations, fraud detection, credit risk analysis, customer retention prediction, analyzing item layouts in stores, and many more. . Using Cloud AI Platform Pipelines or Kubeflow Pipelines to orchestrate a Tables workflow . You can run this example via a Cloud AI Platform Pipelines installation, or via Kubeflow Pipelines on a Kubeflow on GKE installation. Cloud AI Platform Pipelines was recently launched in Beta. Slightly different variants of the pipeline specification are required depending upon which you’re using. (It would be possible to run the example on other Kubeflow installations too, but that would require additional credentials setup not covered in this tutorial). . Install a Cloud AI Platform Pipelines cluster . You can create an AI Platform Pipelines installation with a few clicks. Access AI Platform Pipelines by visiting the AI Platform Panel in the Cloud Console. . Create a new Pipelines instance. See the documentation for more detail. . (You can also do this installation from the command line onto an existing GKE cluster if you prefer. If you do, for consistency with the UI installation, create the GKE cluster with --scopes cloud-platform). . Or, install Kubeflow to use Kubeflow Pipelines . You can also run this example from a Kubeflow installation. For the example to work out of the box, you’ll need a Kubeflow on GKE installation, set up to use IAP. An easy way to do this is via the Kubeflow ‘click to deploy’ web app, or you can follow the command-line instructions here. . Upload and run the Tables end-to-end Pipeline . Once a Pipelines installation is running, we can upload the example AutoML Tables pipeline. Click on Pipelines in the left nav bar of the Pipelines Dashboard. Click on Upload Pipeline. . For Cloud AI Platform Pipelines, upload tables_pipeline_caip.py.tar.gz. This archive points to the compiled version of this pipeline, specified and compiled using the Kubeflow Pipelines SDK. | For Kubeflow Pipelines on a Kubeflow installation, upload tables_pipeline_kf.py.tar.gz. This archive points to the compiled version of this pipeline. To run this example on a KF installation, you will need to give the &lt;deployment-name&gt;-user@&lt;project-id&gt;.iam.gserviceaccount.com service account AutoML Admin privileges. | . Note: The difference between the two pipelines relates to how GCP authentication is handled. For the Kubeflow pipeline, we’ve added .apply(gcp.use_gcp_secret(&#39;user-gcp-sa&#39;)) annotations to the pipeline steps. This tells the pipeline to use the mounted secret—set up during the installation process— that provides GCP account credentials. With the Cloud AI Platform Pipelines installation, the GKE cluster nodes have been set up to use the cloud-platform scope. With recent Kubeflow releases, use of the mounted secret is no longer necessary, but we include both versions for compatibility. . The uploaded pipeline graph will look similar to this: . The uploaded Tables &#39;end-to-end&#39; pipeline. Click the +Create Run button to run the pipeline. You will need to fill in some pipeline parameters. Specifically, replace YOUR_PROJECT_HERE with the name of your project; replace YOUR_DATASET_NAME with the name you want to give your new dataset (make it unique, and use letters, numbers and underscores up to 32 characters); and replace YOUR_BUCKET_NAME with the name of a GCS bucket. Do not include the gs:// prefix— just enter the name. This bucket should be in the same region as that specified by the gcp_region parameter. E.g., if you keep the default us-central1 region, your bucket should also be a regional (not multi-regional) bucket in the us-central1 region. ++double check that this is necessary.++ . If you want to schedule a recurrent set of runs, you can do that instead. If your data is in BigQuery— as is the case for this example pipeline— and has a temporal aspect, you could define a view to reflect that, e.g. to return data from a window over the last N days or hours. Then, the AutoML pipeline could specify ingestion of data from that view, grabbing an updated data window each time the pipeline is run, and building a new model based on that updated window. . The steps executed by the pipeline . The example pipeline creates a dataset, imports data into the dataset from a BigQuery view, and trains a custom model on that data. Then, it fetches evaluation and metrics information about the trained model, and based on specified criteria about model quality, uses that information to automatically determine whether to deploy the model for online prediction. We’ll take a closer look at each of the pipeline steps, and how they’re implemented. . Create a Tables dataset and adjust its schema . This pipeline creates a new Tables dataset, and ingests data from a BigQuery table for the “bikes and weather” dataset described above. These actions are implemented by the first two steps in the pipeline (the automl-create-dataset-for-tables and automl-import-data-for-tables steps). . While we’re not showing it in this example, AutoML Tables supports ingestion from BigQuery views as well as tables. This can be an easy way to do feature engineering: leverage BigQuery’s rich set of functions and operators to clean and transform your data before you ingest it. . When the data is ingested, AutoML Tables infers the data type for each field (column). In some cases, those inferred types may not be what you want. For example, for our “bikes and weather” dataset, several ID fields (like the rental station IDs) are set by default to be numeric, but we want them treated as categorical when we train our model. In addition, we want to treat the loc_cross strings as categorical rather than text. . We make these adjustments programmatically, by defining a pipeline parameter that specifies the schema changes we want to make. Then, in the automl-set-dataset-schema pipeline step, for each indicated schema adjustment, we call update_column_spec: . client.update_column_spec( dataset_display_name=dataset_display_name, column_spec_display_name=column_spec_display_name, type_code=type_code, nullable=nullable ) . Before we can train the model, we also need to specify the target column— what we want our model to predict. In this case, we’ll train the model to predict rental duration. This is a numeric value, so we’ll be training a regression model. . client.set_target_column( dataset_display_name=dataset_display_name, column_spec_display_name=target_column_spec_name ) . Train a custom model on the dataset . Once the dataset is defined and its schema set properly, the pipeline will train the model. This happens in the automl-create-model-for-tables pipeline step. Via pipeline parameters, we can specify the training budget, the optimization objective (if not using the default), and can additionally specify which columns to include or exclude from the model inputs. . You may want to specify a non-default optimization objective depending upon the characteristics of your dataset. This table describes the available optimization objectives and when you might want to use them. For example, if you were training a classification model using an imbalanced dataset, you might want to specify use of AUC PR (MAXIMIZE_AU_PRC), which optimizes results for predictions for the less common class. . client.create_model( model_display_name, train_budget_milli_node_hours=train_budget_milli_node_hours, dataset_display_name=dataset_display_name, optimization_objective=optimization_objective, include_column_spec_names=include_column_spec_names, exclude_column_spec_names=exclude_column_spec_names, ) . View model search information via Cloud Logging . You can view details about an AutoML Tables model via Cloud Logging. Using Logging, you can see the final model hyperparameters as well as the hyperparameters and object values used during model training and tuning. . An easy way to access these logs is to go to the AutoML Tables page in the Cloud Console. Select the Models tab in the left navigation pane and click on the model you’re interested in. Click the “Model” link to see the final hyperparameter logs. To see the tuning trial hyperparameters, click the “Trials” link. . View a model&#39;s search logs from its evaluation information. For example, here is a look at the Trials logs a custom model trained on the “bikes and weather” dataset, with one of the entries expanded in the logs: . The &#39;Trials&#39; logs for a &quot;bikes and weather&quot; model Custom model evaluation . Once your custom model has finished training, the pipeline moves on to its next step: model evaluation. We can access evaluation metrics via the API. We’ll use this information to decide whether or not to deploy the model. . These actions are factored into two steps. The process of fetching the evaluation information can be a general-purpose component (pipeline step) used in many situations; and then we’ll follow that with a more special-purpose step, that analyzes that information and uses it to decide whether or not to deploy the trained model. . In the first of these pipeline steps— the automl-eval-tables-model step— we’ll retrieve the evaluation and global feature importance information. . model = client.get_model(model_display_name=model_display_name) feat_list = [(column.feature_importance, column.column_display_name) for column in model.tables_model_metadata.tables_model_column_info] evals = list(client.list_model_evaluations(model_display_name=model_display_name)) . AutoML Tables automatically computes global feature importance for a trained model. This shows, across the evaluation set, the average absolute attribution each feature receives. Higher values mean the feature generally has greater influence on the model’s predictions. This information is useful for debugging and improving your model. If a feature’s contribution is negligible—if it has a low value—you can simplify the model by excluding it from future training. The pipeline step renders the global feature importance data as part of the pipeline run’s output: . Global feature importance for the model inputs, rendered by a Kubeflow Pipeline step. For our example, based on the graphic above, we might try training a model without including bike_id. . In the following pipeline step— the automl-eval-metrics step— the evaluation output from the previous step is grabbed as input, and parsed to extract metrics that we’ll use in conjunction with pipeline parameters to decide whether or not to deploy the model. One of the pipeline input parameters allows specification of metric thresholds. In this example, we’re training a regression model, and we’re specifying a mean_absolute_error (MAE) value as a threshold in the pipeline input parameters: . {&quot;mean_absolute_error&quot;: 450} . The pipeline step compares the model evaluation information to the given threshold constraints. In this case, if the MAE is &lt; 450, the model will not be deployed. The pipeline step outputs that decision, and displays the evaluation information it’s using as part of the pipeline run’s output: . Information about a model&#39;s evaluation, rendered by a Kubeflow Pipeline step. (Conditional) model deployment . You can deploy any of your custom Tables models to make them accessible for online prediction requests. The pipeline code uses a conditional test to determine whether or not to run the step that deploys the model, based on the output of the evaluation step described above: . with dsl.Condition(eval_metrics.outputs[&#39;deploy&#39;] == True): deploy_model = deploy_model_op( ... ) . Only if the model meets the given criteria, will the deployment step (called automl-deploy-tables-model) be run, and the model be deployed automatically as part of the pipeline run: . response = client.deploy_model(model_display_name=model_display_name) . You can always deploy a model later if you like. . Putting it together: The full pipeline execution . The figure below shows the result of a pipeline run. In this case, the conditional step was executed— based on the model evaluation metrics— and the trained model was deployed. Via the UI, you can view outputs, logs for each step, run artifacts and lineage information, and more. See this post for more detail. . ++TODO: replace the following figure with something better++ . Execution of a pipeline run. You can view outputs, logs for each step, run artifacts and lineage information, and more. Getting explanations about your model’s predictions . Once a model is deployed, you can request predictions from that model. You can additionally request explanations for local feature importance: a score showing how much (and in which direction) each feature influenced the prediction for a single example. See this blog post for more information on how those values are calculated. . Here is a notebook example of how to request a prediction and its explanation using the Python client libraries. . from google.cloud import automl_v1beta1 as automl client = automl.TablesClient(project=PROJECT_ID, region=REGION) response = client.predict( model_display_name=model_display_name, inputs=inputs, feature_importance=True, ) . The prediction response will have a structure like this. (The notebook above shows how to visualize the local feature importance results using matplotlib.) . It’s easy to explore local feature importance through the Cloud Console’s AutoML Tables UI as well. After you deploy a model, go to the TEST &amp; USE tab of the Tables panel, select ONLINE PREDICTION, enter the field values for the prediction, and then check the Generate feature importance box at the bottom of the page. The result will show the feature importance values as well as the prediction. This blog post gives some examples of how these explanations can be used to find potential issues with your data or help you better understand your problem domain. . The AutoML Tables UI in the Cloud Console . With this example we’ve focused on how you can automate a Tables workflow using Kubeflow pipelines and the Python client libraries. . All of the pipeline steps can also be accomplished via the AutoML Tables UI in the Cloud Console, including many useful visualizations, and other functionality not implemented by this example pipeline— such as the ability to export the model’s test set and prediction results to BigQuery for further analysis. . Export the trained model and serve it on a GKE cluster . Recently, Tables launched a feature to let you export your full custom model, packaged so that you can serve it via a Docker container. (Under the hood, it is using TensorFlow Serving). This lets you serve your models anywhere that you can run a container, including a GKE cluster. This means that you can run a model serving service on your AI Platform Pipelines or Kubeflow installation, both of which run on GKE. . This blog post walks through the steps to serve the exported model (in this case, using Cloud Run). Follow the instructions in the post through the “View information about your exported model in TensorBoard” section. Here, we’ll diverge from the rest of the post and create a GKE service instead. . Make a copy of deploy_model_for_tables/model_serve_template.yaml file and name it model_serve.yaml. Edit this new file, replacing MODEL_NAME with some meaningful name for your model, IMAGE_NAME with the name of the container image you built (as described in the blog post, and NAMESPACE with the namespace in which you want to run your service (e.g. default). . Then, from the command line, run: . kubectl apply -f model_serve.yaml . to set up your model serving service and its underlying deployment. (Before you do that, make sure that kubectl is set to use your GKE cluster’s credentials. One way to do that is to visit the GKE panel in the Cloud Console, and click Connect for that cluster.) . You can later take down the service and its deployment by running: . kubectl delete -f model_serve.yaml . Send prediction requests to your deployed model service . Once your model serving service is deployed, you can send prediction requests to it. Because we didn’t set up an external endpoint for our service in this simple example, we’ll connect to the service via port forwarding. From the command line, run the following, replacing &lt;your-model-name&gt; with the value you replaced MODEL_NAME by, when creating your yaml file, and &lt;service-namespace&gt; with the namespace in which your service is running— the same namespace value you used in the yaml file. . kubectl -n &lt;service-namespace&gt; port-forward svc/&lt;your-model-name&gt; 8080:80 . Then, from the deploy_model_for_tables directory, send a prediction request to your service like this: . curl -X POST --data @./instances.json http://localhost:8080/predict . You should see a result like this, with a prediction for each instance in the instances.json file: . {&quot;predictions&quot;: [860.79833984375, 460.5323486328125, 1211.7664794921875]} . (If you get an error, make sure you’re in the correct directory and see the instances.json file listed). . Note: it would be possible to add this deployment step to the pipeline too. (See deploy_model_for_tables/exported_model_deploy.py). However, the Python client library does not yet support the ‘export’ operation. Once deployment is supported by the client library, this would be a natural addition to the workflow. While not tested, it should also be possible to do the export programmatically via the REST API. . A deeper dive into the pipeline code . The updated Tables Python client library makes it very straightforward to build the Pipelines components that support each stage of the workflow. Kubeflow Pipeline steps are container-based, so that any action you can support via a Docker container image can become a pipeline step. That doesn’t mean that an end-user necessarily needs to have Docker installed. For many straightforward cases, building your pipeline steps . Using the ‘lightweight python components’ functionality to build pipeline steps . For most of the components in this example, we’re building them using the “lightweight python components” functionality as shown in this example notebook, including compilation of the code into a component package. This feature allows you to create components based on Python functions, building on an appropriate base image, so that you do not need to have docker installed or rebuild a container image each time your code changes. . Each component’s python file includes a function definition, and then a func_to_container_op call, passing the function definition, to generate the component’s yaml package file. As we’ll see below, these component package files make it very straightforward to put these steps together to form a pipeline. . The deploy_model_for_tables/tables_deploy_component.py file is representative. It contains an automl_deploy_tables_model function definition. . def automl_deploy_tables_model( gcp_project_id: str, gcp_region: str, model_display_name: str, api_endpoint: str = None, ) -&gt; NamedTuple(&#39;Outputs&#39;, [(&#39;model_display_name&#39;, str), (&#39;status&#39;, str)]): ... return (model_display_name, status) . The function defines the component’s inputs and outputs, and this information will be used to support static checking when we compose these components to build the pipeline. . To build the component yaml file corresponding to this function, we add the following to the components’ Python script, then can run python &lt;filename&gt;.py from the command line to generate it (you must have the Kubeflow Pipelines (KFP) sdk installed). . if __name__ == &#39;__main__&#39;: import kfp kfp.components.func_to_container_op( automl_deploy_tables_model, output_component_file=&#39;tables_deploy_component.yaml&#39;, base_image=&#39;python:3.7&#39;) . Whenever you change the python function definition, just recompile to regenerate the corresponding component file. . Specifying the Tables pipeline . With the components packaged into yaml files, it becomes very straightforward to specify a pipeline, such as tables_pipeline_caip.py, that uses them. Here, we’re just using the load_component_from_file() method, since the yaml files are all local (in the same repo). However, there is also a load_component_from_url() method, which makes it easy to share components. (If your URL points to a file in GitHub, be sure to use raw mode). . create_dataset_op = comp.load_component_from_file( &#39;./create_dataset_for_tables/tables_component.yaml&#39;) import_data_op = comp.load_component_from_file( &#39;./import_data_from_bigquery/tables_component.yaml&#39;) set_schema_op = comp.load_component_from_file( &#39;./import_data_from_bigquery/tables_schema_component.yaml&#39;) train_model_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_component.yaml&#39;) eval_model_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_eval_component.yaml&#39;) eval_metrics_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_eval_metrics_component.yaml&#39;) deploy_model_op = comp.load_component_from_file( &#39;./deploy_model_for_tables/tables_deploy_component.yaml&#39;) . Once all our pipeline ops (steps) are defined using the component definitions, then we can specify the pipeline by calling the constructors, e.g.: . create_dataset = create_dataset_op( gcp_project_id=gcp_project_id, gcp_region=gcp_region, dataset_display_name=dataset_display_name, api_endpoint=api_endpoint, ) . If a pipeline component has been defined to have outputs, other components can access those outputs. E.g., here, the ‘eval’ step is grabbing an output from the ‘train’ step, specifically information about the model display name: . eval_model = eval_model_op( gcp_project_id=gcp_project_id, gcp_region=gcp_region, bucket_name=bucket_name, gcs_path=&#39;automl_evals/{}&#39;.format(dsl.RUN_ID_PLACEHOLDER), api_endpoint=api_endpoint, model_display_name=train_model.outputs[&#39;model_display_name&#39;] ) . In this manner it is straightforward to put together a pipeline from your component definitions. Just don’t forget to recompile the pipeline script (to generate its corresponding .tar.gz archive) if any of its component definitions changed, e.g. python tables_pipeline_caip.py. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/kfp/automl/2020/04/22/automltables_kfp_e2e.html",
            "relUrl": "/ml/kfp/automl/2020/04/22/automltables_kfp_e2e.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting explanations for AutoML Tables predictions",
            "content": "Introduction . Google Cloud’s AutoML Tables lets you automatically build and deploy state-of-the-art machine learning models using your own structured data. . AutoML Tables now has an easier-to-use Tables-specific Python client library, as well as a new ability to explain online prediction results— called local feature importance— which gives visibility into how the features in a specific prediction request informed the resulting prediction. You can read more about explainable AI for Tables in this blog post. . The source for this post is a Jupyter notebook. In this notebook, we&#39;ll create a custom Tables model to predict duration of London bike rentals given information about local weather as well as info about the rental trip. We&#39;ll walk through examples of using the Tables client libraries for creating a dataset, training a custom model, deploying the model, and using it to make predictions; and show how you can programmatically request local feature importance information. . We recommend running this notebook using AI Platform Notebooks. If you want to run the notebook on colab (or locally), it&#39;s possible, but you&#39;ll need to do a bit more setup. See the Appendix section of this notebook for details. . Before you begin . Follow the AutoML Tables documentation to: . Select or create a GCP project. | Make sure that billing is enabled for your project | Enable the Cloud AutoML and Storage APIs. | (Recommended) Create an AI Platform Notebook instance and upload this notebook to it. | . (See also the Quickstart guide for a getting-started walkthrough on AutoML Tables). . Then, install the AutoML Python client libraries into your notebook environment: . !pip3 install -U google-cloud-automl . You may need to restart your notebook kernel after running the above to pick up the installation. . Enter your GCP project ID in the cell below, then run the cell. . PROJECT_ID = &quot;&lt;your-project-id&gt;&quot; . Do some imports . Next, import some libraries and set some variables. . import argparse import os from google.api_core.client_options import ClientOptions from google.cloud import automl_v1beta1 as automl import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types . REGION = &#39;us-central1&#39; DATASET_NAME = &#39;bikes-weather&#39; BIGQUERY_PROJECT_ID = &#39;aju-dev-demos&#39; DATASET_ID = &#39;london_bikes_weather&#39; TABLE_ID = &#39;bikes_weather&#39; IMPORT_URI = &#39;bq://%s.%s.%s&#39; % (BIGQUERY_PROJECT_ID, DATASET_ID, TABLE_ID) print(IMPORT_URI) . DATASET_NAME = &#39;bikes_weather&#39; . Create a dataset, and import data . Next, we&#39;ll define some utility functions to create a dataset, and to import data into a dataset. The client.import_data() call returns an operation future that can be used to check for completion synchronously or asynchronously— in this case we wait synchronously. . def create_dataset(client, dataset_display_name): &quot;&quot;&quot;Create a dataset.&quot;&quot;&quot; # Create a dataset with the given display name dataset = client.create_dataset(dataset_display_name) # Display the dataset information. print(&quot;Dataset name: {}&quot;.format(dataset.name)) print(&quot;Dataset id: {}&quot;.format(dataset.name.split(&quot;/&quot;)[-1])) print(&quot;Dataset display name: {}&quot;.format(dataset.display_name)) print(&quot;Dataset metadata:&quot;) print(&quot; t{}&quot;.format(dataset.tables_dataset_metadata)) print(&quot;Dataset example count: {}&quot;.format(dataset.example_count)) print(&quot;Dataset create time:&quot;) print(&quot; tseconds: {}&quot;.format(dataset.create_time.seconds)) print(&quot; tnanos: {}&quot;.format(dataset.create_time.nanos)) return dataset . def import_data(client, dataset_display_name, path): &quot;&quot;&quot;Import structured data.&quot;&quot;&quot; response = None if path.startswith(&#39;bq&#39;): response = client.import_data( dataset_display_name=dataset_display_name, bigquery_input_uri=path ) else: # Get the multiple Google Cloud Storage URIs. input_uris = path.split(&quot;,&quot;) response = client.import_data( dataset_display_name=dataset_display_name, gcs_input_uris=input_uris ) print(&quot;Processing import...&quot;) # synchronous check of operation status. print(&quot;Data imported. {}&quot;.format(response.result())) . Next, we&#39;ll create the client object that we&#39;ll use for all our operations. . client = automl.TablesClient(project=PROJECT_ID, region=REGION) . Create the Tables dataset: . create_dataset(client, DATASET_NAME) . ... and then import data from the BigQuery table into the dataset. The import command will take a while to run. Wait until it has returned before proceeding. You can also check import status in the Cloud Console. . (Note that if you run this notebook multiple times, you will get an error if you try to create multiple datasets with the same name. However, you can train multiple models against the same dataset.) . import_data(client, DATASET_NAME, IMPORT_URI) . Update the dataset schema . Now we&#39;ll define utility functions to update dataset and column information. We need these to set the dataset&#39;s target column (the field we&#39;ll train our model to predict) and to change the types of some of the columns. AutoML Tables is pretty good at inferring reasonable column types based on input, but in our case, there are some columns (like bike station IDs) that we want to treat as Categorical instead of Numeric. . def update_column_spec(client, dataset_display_name, column_spec_display_name, type_code, nullable=None): &quot;&quot;&quot;Update column spec.&quot;&quot;&quot; response = client.update_column_spec( dataset_display_name=dataset_display_name, column_spec_display_name=column_spec_display_name, type_code=type_code, nullable=nullable ) # synchronous check of operation status. print(&quot;Table spec updated. {}&quot;.format(response)) def update_dataset(client, dataset_display_name, target_column_spec_name=None, time_column_spec_name=None, test_train_column_spec_name=None): &quot;&quot;&quot;Update dataset.&quot;&quot;&quot; if target_column_spec_name is not None: response = client.set_target_column( dataset_display_name=dataset_display_name, column_spec_display_name=target_column_spec_name ) print(&quot;Target column updated. {}&quot;.format(response)) if time_column_spec_name is not None: response = client.set_time_column( dataset_display_name=dataset_display_name, column_spec_display_name=time_column_spec_name ) print(&quot;Time column updated. {}&quot;.format(response)) . def list_column_specs(client, dataset_display_name, filter_=None): &quot;&quot;&quot;List all column specs.&quot;&quot;&quot; result = [] # List all the table specs in the dataset by applying filter. response = client.list_column_specs( dataset_display_name=dataset_display_name, filter_=filter_) print(&quot;List of column specs:&quot;) for column_spec in response: # Display the column_spec information. print(&quot;Column spec name: {}&quot;.format(column_spec.name)) print(&quot;Column spec id: {}&quot;.format(column_spec.name.split(&quot;/&quot;)[-1])) print(&quot;Column spec display name: {}&quot;.format(column_spec.display_name)) print(&quot;Column spec data type: {}&quot;.format(column_spec.data_type)) result.append(column_spec) return result . Update the dataset to indicate that the target column is duration. . update_dataset(client, DATASET_NAME, target_column_spec_name=&#39;duration&#39;, # time_column_spec_name=&#39;ts&#39; ) . Now we&#39;ll update some of the column types. You can list their default specs first if you like: . list_column_specs(client, DATASET_NAME) . ... and now we&#39;ll update them to the types we want: . update_column_spec(client, DATASET_NAME, &#39;end_station_id&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;start_station_id&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;loc_cross&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;bike_id&#39;, &#39;CATEGORY&#39;) . You can view the results in the Cloud Console. Note that useful stats are generated for each column. You can also run the list_column_specs() function again to see the new config. . # list_column_specs(client, DATASET_NAME) . Train a custom model on the dataset . Now we&#39;re ready to train a model on the dataset. We&#39;ll need to generate a unique name for the model, which we&#39;ll do by appending a timestamp, in case you want to run this notebook multiple times. The 1000 arg in the create_model() call specifies to budget 1 hour of training time. . In the create_model() utility function below, we may not want to block on the result, since total job time can be multiple hours. If you want the function to block until training is complete, uncomment the last line of the function below. . import time MODEL_NAME = &#39;bwmodel_&#39; + str(int(time.time())) print(&#39;MODEL_NAME: %s&#39; % MODEL_NAME) def create_model(client, dataset_display_name, model_display_name, train_budget_milli_node_hours, include_column_spec_names=None, exclude_column_spec_names=None): &quot;&quot;&quot;Create a model.&quot;&quot;&quot; # Create a model with the model metadata in the region. response = client.create_model( model_display_name, train_budget_milli_node_hours=train_budget_milli_node_hours, dataset_display_name=dataset_display_name, include_column_spec_names=include_column_spec_names, exclude_column_spec_names=exclude_column_spec_names, ) print(&quot;Training model...&quot;) print(&quot;Training operation: {}&quot;.format(response.operation)) print(&quot;Training operation name: {}&quot;.format(response.operation.name)) # uncomment the following to block until training is finished. # print(&quot;Training completed: {}&quot;.format(response.result())) . create_model(client, DATASET_NAME, MODEL_NAME, 1000) . Get the status of your training job . Edit the following call to set OP_NAME to the &quot;training operation name&quot; listed in the output of create_model() above. . OP_NAME = &#39;YOUR TRAINING OPERATION NAME&#39; . def get_operation_status(client, operation_full_id): &quot;&quot;&quot;Get operation status.&quot;&quot;&quot; # Get the latest state of a long-running operation. op = client.auto_ml_client.transport._operations_client.get_operation( operation_full_id ) print(&quot;Operation status: {}&quot;.format(op)) from google.cloud.automl import types msg = types.OperationMetadata() print(msg.ParseFromString(op.metadata.value)) . The training job may take several hours. You can check on its status in the Cloud Console UI. You can also monitor it via the get_operation_status() call below. (Make sure you&#39;ve edited the OP_NAME variable value above). You&#39;ll see: done: true in the output when it&#39;s finished. . (Note: if you should lose your notebook kernel context while the training job is running, you can continue the rest of the notebook later with a new kernel: just make note of the MODEL_NAME. You can find that information in the Cloud Console as well). . res = get_operation_status(client, OP_NAME) . Get information about your trained custom model . Once it has been created, you can get information about a specific model. (While the training job is still running, you&#39;ll just get a not found message.) . from google.cloud.automl_v1beta1 import enums from google.api_core import exceptions def get_model(client, model_display_name): &quot;&quot;&quot;Get model details.&quot;&quot;&quot; try: model = client.get_model(model_display_name=model_display_name) except exceptions.NotFound: print(&quot;Model %s not found.&quot; % model_display_name) return (None, None) # Get complete detail of the model.a model = client.get_model(model_display_name=model_display_name) # Retrieve deployment state. if model.deployment_state == enums.Model.DeploymentState.DEPLOYED: deployment_state = &quot;deployed&quot; else: deployment_state = &quot;undeployed&quot; # get features of top global importance feat_list = [ (column.feature_importance, column.column_display_name) for column in model.tables_model_metadata.tables_model_column_info ] feat_list.sort(reverse=True) if len(feat_list) &lt; 10: feat_to_show = len(feat_list) else: feat_to_show = 10 # Display the model information. print(&quot;Model name: {}&quot;.format(model.name)) print(&quot;Model id: {}&quot;.format(model.name.split(&quot;/&quot;)[-1])) print(&quot;Model display name: {}&quot;.format(model.display_name)) print(&quot;Features of top importance:&quot;) for feat in feat_list[:feat_to_show]: print(feat) print(&quot;Model create time:&quot;) print(&quot; tseconds: {}&quot;.format(model.create_time.seconds)) print(&quot; tnanos: {}&quot;.format(model.create_time.nanos)) print(&quot;Model deployment state: {}&quot;.format(deployment_state)) return (model, feat_list) . Don&#39;t proceed with the rest of the notebook until the model has finished training and the following get_model() call returns model information rather than &#39;not found&#39;. . Once the training job has finished, we can get information about the model, including information about which input features proved to be the most important globally (that is, across the full training dataset). . (model, global_feat_importance) = get_model(client, MODEL_NAME) . We can graph the global feature importance values to get a visualization of which inputs were most important in training the model. (The Cloud Console UI also displays such a graph). . print(global_feat_importance) . import matplotlib.pyplot as plt res = list(zip(*global_feat_importance)) x = list(res[0]) y = list(res[1]) y_pos = list(range(len(y))) plt.barh(y_pos, x, alpha=0.5) plt.yticks(y_pos, y) plt.show() . See your model&#39;s evaluation metrics . We can also get model evaluation information once the model is trained. The available metrics depend upon which optimization objective you used. In this example, we used the default, RMSE. . evals = client.list_model_evaluations(model_display_name=MODEL_NAME) list(evals)[1].regression_evaluation_metrics . Use your trained model to make predictions and see explanations of the results . Deploy your model and get predictions + explanations . Once your training job has finished, you can use your model to make predictions. . With online prediction, you can now request explanations of the results, in the form of local feature importance calculations on the inputs. Local feature importance gives you visibility into how the features in a specific prediction request informed the resulting prediction. . To get online predictions, we first need to deploy the model. . Note: see the documentation for other prediction options including the ability to export your custom model and run it in a container anywhere. . def deploy_model(client, model_display_name): &quot;&quot;&quot;Deploy model.&quot;&quot;&quot; response = client.deploy_model(model_display_name=model_display_name) # synchronous check of operation status. print(&quot;Model deployed. {}&quot;.format(response.result())) . It will take a while to deploy the model. Wait for the deploy_model() call to finish before proceeding with the rest of the notebook cells. You can track status in the Console UI as well. . deploy_model(client, MODEL_NAME) . Once the model is deployed, you can access it via the UI, or the API, to make online prediction requests. These can include a request for local feature importance calculations on the inputs, a newly-launched feature. Local feature importance gives you visibility into how the features in a specific prediction request informed the resulting prediction. . def predict(client, model_display_name, inputs, feature_importance=False): &quot;&quot;&quot;Make a prediction.&quot;&quot;&quot; if feature_importance: response = client.predict( model_display_name=model_display_name, inputs=inputs, feature_importance=True, ) else: response = client.predict( model_display_name=model_display_name, inputs=inputs) print(&quot;Prediction results:&quot;) print(response) return response . inputs = { &quot;bike_id&quot;: &quot;5373&quot;, &quot;day_of_week&quot;: &quot;3&quot;, &quot;end_latitude&quot;: 51.52059681, &quot;end_longitude&quot;: -0.116688468, &quot;end_station_id&quot;: &quot;68&quot;, &quot;euclidean&quot;: 3589.5146210024977, &quot;loc_cross&quot;: &quot;POINT(-0.07 51.52)POINT(-0.12 51.52)&quot;, &quot;max&quot;: 44.6, &quot;min&quot;: 34.0, &quot;prcp&quot;: 0, &quot;ts&quot;: &quot;1480407420&quot;, &quot;start_latitude&quot;: 51.52388, &quot;start_longitude&quot;: -0.065076, &quot;start_station_id&quot;: &quot;445&quot;, &quot;temp&quot;: 38.2, &quot;dewp&quot;: 28.6 } . Try running the prediction request first without, then with, the local feature importance calculations, to see the difference in the information that is returned. (The actual duration— that we&#39;re predicting— is 1200.) . predict(client, MODEL_NAME, inputs, feature_importance=False) . response = predict(client, MODEL_NAME, inputs, feature_importance=True) . We can plot the local feature importance values to get a visualization of which fields were most and least important for this particular prediction. . import matplotlib.pyplot as plt col_info = response.payload[0].tables.tables_model_column_info x = [] y = [] for c in col_info: y.append(c.column_display_name) x.append(c.feature_importance) y_pos = list(range(len(y))) plt.barh(y_pos, x, alpha=0.5) plt.yticks(y_pos, y) plt.show() . You can see a similar graphic in the Cloud Console Tables UI when you submit an ONLINE PREDICTION and tick the &quot;Generate feature importance&quot; checkbox. . The local feature importance calculations are specific to a given input instance. . Summary . In this notebook, we showed how you can use the AutoML Tables client library to create datasets, train models, and get predictions from your trained model— and in particular, how you can get explanations of the results along with the predictions. . Appendix: running this notebook on colab (or locally) . It&#39;s possible to run this example on colab, but it takes a bit more setup. Do the following before you create the Tables client object or call the API. . Create a service account, give it the necessary roles (e.g., AutoML Admin) and download a json credentials file for the service account. Upload the credentials file to the colab file system. . Then, edit the following to point to that file, and run the cell: . %env GOOGLE_APPLICATION_CREDENTIALS /content/your-credentials-file.json . Your Tables API calls should now be properly authenticated. If you lose the colab runtime, you&#39;ll need to re-upload the file and re-set the environment variable. . If you&#39;re running the notebook locally, point the GOOGLE_APPLICATION_CREDENTIALS environment variable to the service account credentials file before starting the notebook, e.g.: . export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your-credentials-file.json .",
            "url": "https://amygdala.github.io/gcp_blog/ml/xai/automl/jupyter/2020/04/17/automl_tables_xai.html",
            "relUrl": "/ml/xai/automl/jupyter/2020/04/17/automl_tables_xai.html",
            "date": " • Apr 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://amygdala.github.io/gcp_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amygdala.github.io/gcp_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
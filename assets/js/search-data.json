{
  
    
        "post0": {
            "title": "Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift",
            "content": "Introduction . With ML workflows, it is often insufficient to train and deploy a given model just once. Even if the model has desired accuracy initially, this can change if the data used for making prediction requests becomes— perhaps over time— sufficiently different from the data used to originally train the model. . When new data becomes available, which could be used for retraining a model, it can be helpful to apply techniques for analyzing data ‘drift’, and determining whether the drift is sufficiently anomalous to warrant retraining yet. It can also be useful to trigger such an analysis— and potential re-run of your training pipeline— automatically, upon arrival of new data. . This blog post highlights an example notebook that shows how to set up such a scenario with Kubeflow Pipelines (KFP). It shows how to build a pipeline that checks for statistical drift across successive versions of a dataset and uses that information to make a decision on whether to (re)train a model1; and how to configure event-driven deployment of pipeline jobs when new data arrives. . The notebook builds on an example highlighted in a previous blog post — which shows a KFP training and serving pipeline— and introduces two primary new concepts: . the example demonstrates use of the TensorFlow Data Validation (TFDV) library to build pipeline components that derive dataset statistics and detect drift between older and newer dataset versions, and shows how to use drift information to decide whether to retrain a model on newer data. | the example shows how to support event-triggered launch of Kubeflow Pipelines runs from a Cloud Functions (GCF) function, where the Function run is triggered by addition of a file to a given Cloud Storage (GCS) bucket. | . The machine learning task uses a tabular dataset that joins London bike rental information with weather data, and train a Keras model to predict rental duration. See this and this blog post and associated README for more background on the dataset and model architecture. . A pipeline run using TFDV-based components to detect &#39;data drift&#39;. Running the example notebook . The example notebook requires a Google Cloud Platform (GCP) account and project, ideally with quota for using GPUs, and— as detailed in the notebook— an installation of AI Platform Pipelines (Hosted Kubeflow Pipelines) (that is, an installation of KFP on Google Kubernetes Engine (GKE)), with a few additional configurations once installation is complete. . The notebook can be run using either Colab or AI Platform Notebooks. . Creating TFDV-based KFP components . Our first step is to build the TFDV components that we want to use in our pipeline. . Note: For this example, our training data is in GCS, in CSV-formatted files. So, we can take advantage of TFDV’s ability to process CSV files. The TFDV libraries can also process files in TFRecords format. . We’ll define both TFDV KFP pipeline components as ‘lightweight’ Python-function-based components. For each component, we define a function, then call kfp.components.func_to_container_op() on that function to build a reusable component in .yaml format. Let’s take a closer look at how this works (details are in the notebook). . Below is the Python function we’ll use to generate TFDV statistics from a collection of csv files. The function— and the component we’ll create from it— outputs the path to the generated stats file. When we define a pipeline that uses this component, we’ll use this step’s output as input to another pipeline step. TFDV uses a Beam pipeline— not to be confused with KFP Pipelines— to implement the stats generation. Depending upon configuration, the component can use either the Direct (local) runner or the Dataflow runner. Running the Beam pipeline on Dataflow rather than locally can make sense with large datasets. . from typing import NamedTuple def generate_tfdv_stats(input_data: str, output_path: str, job_name: str, use_dataflow: str, project_id: str, region:str, gcs_temp_location: str, gcs_staging_location: str, whl_location: str = &#39;&#39;, requirements_file: str = &#39;requirements.txt&#39; ) -&gt; NamedTuple(&#39;Outputs&#39;, [(&#39;stats_path&#39;, str)]): import logging import time import tensorflow_data_validation as tfdv import tensorflow_data_validation.statistics.stats_impl from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions logging.getLogger().setLevel(logging.INFO) logging.info(&quot;output path: %s&quot;, output_path) logging.info(&quot;Building pipeline options&quot;) # Create and set your PipelineOptions. options = PipelineOptions() if use_dataflow == &#39;true&#39;: logging.info(&quot;using Dataflow&quot;) if not whl_location: logging.warning(&#39;tfdv whl file required with dataflow runner.&#39;) exit(1) google_cloud_options = options.view_as(GoogleCloudOptions) google_cloud_options.project = project_id google_cloud_options.job_name = &#39;{}-{}&#39;.format(job_name, str(int(time.time()))) google_cloud_options.staging_location = gcs_staging_location google_cloud_options.temp_location = gcs_temp_location google_cloud_options.region = region options.view_as(StandardOptions).runner = &#39;DataflowRunner&#39; setup_options = options.view_as(SetupOptions) setup_options.extra_packages = [whl_location] setup_options.requirements_file = &#39;requirements.txt&#39; tfdv.generate_statistics_from_csv( data_location=input_data, output_path=output_path, pipeline_options=options) return (output_path, ) . To turn this function into a KFP component, we’ll call kfp.components.func_to_container_op(). We’re passing it a base container image to use: gcr.io/google-samples/tfdv-tests:v1. This base image has the TFDV libraries already installed, so that we don’t need to install them ‘inline’ when we run a pipeline step based on this component. . import kfp kfp.components.func_to_container_op(generate_tfdv_stats, output_component_file=&#39;tfdv_component.yaml&#39;, base_image=&#39;gcr.io/google-samples/tfdv-tests:v1&#39;) . We’ll take the same approach to build a second TFDV-based component, one which detects drift between datasets by comparing their stats. The TFDV library makes this straightforward. We’re using a drift comparator appropriate for a regression model— as used in the example pipeline— and looking for drift on a given set of fields (in this case, for example purposes, just one). The tensorflow_data_validation.validate_statistics() call will then tell us whether the drift anomaly for that field is over the specified threshold. See the TFDV docs for more detail. . schema1 = tfdv.infer_schema(statistics=stats1) tfdv.get_feature(schema1, &#39;duration&#39;).drift_comparator.jensen_shannon_divergence.threshold = 0.01 drift_anomalies = tfdv.validate_statistics( statistics=stats2, schema=schema1, previous_statistics=stats1) . (The details of this second component definition are in the example notebook). . Defining a pipeline that uses the TFDV components . After we’ve defined both TFDV components— one to generate stats for a dataset, and one to detect drift between datasets— we’re ready to build a Kubeflow Pipeline that uses these components, in conjunction with previously-built components for a training &amp; serving workflow. . Instantiate pipeline ops from the components . KFP components in yaml format are shareable and reusable. We’ll build our pipeline by starting with some already-built components— (described in more detail here)— that support our basic ‘train/evaluate/deploy’ workflow. . We’ll instantiate some pipeline ops from these pre-existing components like this, by loading them via URL: . import kfp.components as comp # pre-existing components train_op = comp.load_component_from_url( &#39;https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/train_component.yaml&#39; ) ... etc. ... . … then create our TFDV ops from the new components we just built: . tfdv_op = comp.load_component_from_file( &#39;tfdv_component.yaml&#39; ) tfdv_drift_op = comp.load_component_from_file( &#39;tfdv_drift_component.yaml&#39; ) . Then, we define a KFP pipeline from the defined ops. We’re not showing the pipeline in full here— see the notebook for details. Two pipeline steps use the tfdv_op, which generates the stats. tfdv1 generates stats for the test data, and tfdv2 for the training data. In the following, you can see that the tfdv_drift step takes as input the output from the tfdv2 (stats for training data) step. . @dsl.pipeline( name=&#39;bikes_weather_tfdv&#39;, description=&#39;Model bike rental duration given weather&#39; ) def bikes_weather_tfdv( ... other pipeline params ... working_dir: str = &#39;gs://YOUR/GCS/PATH&#39;, data_dir: str = &#39;gs://aju-dev-demos-codelabs/bikes_weather/&#39;, project_id: str = &#39;YOUR-PROJECT-ID&#39;, region: str = &#39;us-central1&#39;, requirements_file: str = &#39;requirements.txt&#39;, job_name: str = &#39;test&#39;, whl_location: str = &#39;tensorflow_data_validation-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl&#39;, use_dataflow: str = &#39;&#39;, stats_older_path: str = &#39;gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb&#39; ): ... tfdv1 = tfdv_op( # TFDV stats for the test data input_data=&#39;%stest-*.csv&#39; % (data_dir,), output_path=&#39;%s/tfdv_expers/%s/eval/evaltest.pb&#39; % (working_dir, dsl.RUN_ID_PLACEHOLDER), job_name=&#39;%s-1&#39; % (job_name,), use_dataflow=use_dataflow, project_id=project_id, region=region, gcs_temp_location=&#39;%s/tfdv_expers/tmp&#39; % (working_dir,), gcs_staging_location=&#39;%s/tfdv_expers&#39; % (working_dir,), whl_location=whl_location, requirements_file=requirements_file ) tfdv2 = tfdv_op( # TFDV stats for the training data input_data=&#39;%strain-*.csv&#39; % (data_dir,), # output_path=&#39;%s/%s/eval/evaltrain.pb&#39; % (output_path, dsl.RUN_ID_PLACEHOLDER), output_path=&#39;%s/tfdv_expers/%s/eval/evaltrain.pb&#39; % (working_dir, dsl.RUN_ID_PLACEHOLDER), job_name=&#39;%s-2&#39; % (job_name,), use_dataflow=use_dataflow, project_id=project_id, region=region, gcs_temp_location=&#39;%s/tfdv_expers/tmp&#39; % (working_dir,), gcs_staging_location=&#39;%s/tfdv_expers&#39; % (working_dir,), whl_location=whl_location, requirements_file=requirements_file ) # compare generated training data stats with stats from a previous version # of the training data set. tfdv_drift = tfdv_drift_op(stats_older_path, tfdv2.outputs[&#39;stats_path&#39;]) # proceed with training if drift is detected (or if no previous stats were provided) with dsl.Condition(tfdv_drift.outputs[&#39;drift&#39;] == &#39;true&#39;): train = train_op(...) eval_metrics = eval_metrics_op(...) with dsl.Condition(eval_metrics.outputs[&#39;deploy&#39;] == &#39;deploy&#39;): serve = serve_op(...) . While not all pipeline details are shown, you can see that this pipeline definition includes some conditional expressions; parts of the pipeline will run only if an output of an ‘upstream’ step meets the given conditions. We start the model training step if drift anomalies were detected. (And, once training is completed, we’ll deploy the model for serving only if its evaluation metrics meet certain thresholds). . Here’s the DAG for this pipeline. You can see the conditional expressions reflected; and can see that the step to generates stats for the test dataset provides no downstream dependencies, but the stats on the training set are used as input for the drift detection step. . The pipeline DAG Here’s a pipeline run in progress: . A pipeline run in progress. See the example notebook for more details on how to run this pipeline. . Event-triggered pipeline runs . Once you have defined this pipeline, a next useful step is to automatically run it when an update to the dataset is available, so that each dataset update triggers an analysis of data drift and potential model (re)training. . We’ll show how to do this using Cloud Functions (GCF), by setting up a function that is triggered when new data is added to a GCS bucket. . Set up a GCF function to trigger a pipeline run when a dataset is updated . We’ll define and deploy a Cloud Functions (GCF) function that launches a run of this pipeline when new training data becomes available, as triggered by the creation or modification of a file in a ‘trigger’ bucket on GCS. . In most cases, you don’t want to launch a new pipeline run for every new file added to a dataset— since typically, the dataset will be comprised of a collection of files, to which you will add/update multiple files in a batch. So, you don’t want the ‘trigger bucket’ to be the dataset bucket (if the data lives on GCS)— that will trigger unwanted pipeline runs. Instead, we’ll trigger a pipeline run after the upload of a batch of new data has completed. . To do this, we’ll use an approach where the the ‘trigger’ bucket is different from the bucket used to store dataset files. ‘Trigger files’ uploaded to that bucket are expected to contain the path of the updated dataset as well as the path to the data stats file generated for the last model trained. A trigger file is uploaded once the new data upload has completed, and that upload triggers a run of the GCF function, which in turn reads info on the new data path from the trigger file and launches the pipeline job. . Define the GCF function . To set up this process, we’ll first define the GCF function in a file called main.py, as well as an accompanying requirements file in the same directory that specifies the libraries to load prior to running the function. The requirements file will indicate to install the KFP SDK: . kfp==1.4 . The code looks like this (with some detail removed); we parse the trigger file contents and use that information to launch a pipeline run. The code uses the values of several environment variables that we will set when uploading the GCF function. . import logging import os import kfp from kfp import dsl from kfp import compiler from kfp import components from google.cloud import storage PIPELINE_PROJECT_ID = os.getenv(&#39;PIPELINE_PROJECT_ID&#39;) ...etc... def read_trigger_file(data, context, storage_client): &quot;&quot;&quot;Read the contents of the trigger file and return as string. &quot;&quot;&quot; .... bucket = storage_client.get_bucket(data[&#39;bucket&#39;]) blob = bucket.get_blob(data[&#39;name&#39;]) trigger_file_string = blob.download_as_string().strip() logging.info(&#39;trigger file contents: {}&#39;.format(trigger_file_string)) return trigger_file_string.decode(&#39;UTF-8&#39;) def gcs_update(data, context): &quot;&quot;&quot;Background Cloud Function to be triggered by Cloud Storage. &quot;&quot;&quot; storage_client = storage.Client() # get the contents of the trigger file trigger_file_string = read_trigger_file(data, context, storage_client) trigger_file_info = trigger_file_string.strip().split(&#39; n&#39;) # then run the pipeline using the given job spec, passing the trigger file contents # as parameter values. logging.info(&#39;running pipeline with id %s...&#39;, PIPELINE_ID) # create the client object client = kfp.Client(host=PIPELINE_HOST) # deploy the pipeline run run = client.run_pipeline(EXP_ID, &#39;bw_tfdv_gcf&#39;, pipeline_id=PIPELINE_ID, params={&#39;working_dir&#39;: WORKING_DIR, &#39;project_id&#39;: PIPELINE_PROJECT_ID, &#39;use_dataflow&#39;: USE_DATAFLOW, &#39;data_dir&#39;: trigger_file_info[0], &#39;stats_older_path&#39;: trigger_file_info[1]}) logging.info(&#39;job response: %s&#39;, run) . Then we’ll deploy the GCF function as follows. Note that we’re indicating to use the gcs_update definition (from main.py), and specifying the trigger bucket. Note also how we’re setting environment vars as part of the deployment. . gcloud functions deploy gcs_update --set-env-vars PIPELINE_PROJECT_ID={PROJECT_ID},WORKING_DIR={WORKING_DIR},PIPELINE_SPEC={PIPELINE_SPEC},PIPELINE_ID={PIPELINE_ID},PIPELINE_HOST={PIPELINE_HOST},EXP_ID={EXP_ID},USE_DATAFLOW=true --runtime python37 --trigger-resource {TRIGGER_BUCKET} --trigger-event google.storage.object.finalize . Trigger a pipeline run when new data becomes available . Once the GCF function is set up, it will run when a file is added to (or modified in) the trigger bucket. For this simple example, the GCF function expects trigger files of the following format, where the first line is the path to the updated dataset, and the second line is the path to the TFDV stats for the dataset used for the previously-trained model. More generally, such a trigger file can contain whatever information is necessary to determine how to parameterize the pipeline run. . gs://path/to/new/or/updated/dataset/ gs://path/to/stats/from/previous/dataset/stats.pb . Summary . This blog post showed how to build Kubeflow Pipeline components, using the TFDV libraries, to analyze datasets and detect data drift. Then, it showed how to support event-triggered pipeline runs via GCF. . In this example, we show full model retraining on a new dataset. An alternate scenario— not covered here— could involve tuning an existing model with new data. &#8617; . |",
            "url": "https://amygdala.github.io/gcp_blog/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html",
            "relUrl": "/ml/kfp/mlops/tfdv/gcf/2021/02/26/kfp_tfdv_event_triggered.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Keras Tuner KFP example, part II— creating a lightweight component for metrics evaluation",
            "content": "Introduction . This blog post and accompanying tutorial walked through how to build a Kubeflow Pipelines (KFP) pipeline that uses the Keras Tuner to build a hyperparameter-tuning workflow that uses distributed HP search. . That pipeline does HP tuning, then runs full training on the N best parameter sets identified from the HP search, then deploys the full models to TF-serving. One thing that was missing from that pipeline was any check on the quality of the trained models prior to deployment to TF-Serving. . This post is based on an example notebook, and is a follow-on to that tutorial. You can follow along in the notebook instead if you like (see below). . Here, we’ll show how you can create a KFP “lightweight component”, built from a python function, to do a simple threshold check on some of the model metrics in order to decide whether to deploy the model. (This is a pretty simple approach, that we’re using for illustrative purposes; for production models you’d probably want to do more sophisticated analyses. The TFMA library might be of interest). We’ll also show how to use the KFP SDK to define and run pipelines from a notebook. . Setup . This example assumes that you’ve done the setup indicated in the README, and have an AI Platform Pipelines (Hosted KFP) installation, with GPU node pools added to the cluster. . Create an AI Platform Notebooks instance . In addition, create an AI Platform Notebooks instance on which to run the example notebook on which this post is based. See setup instructions here. (You can run this notebook in other environments, including, locally, but that requires additional auth setup that we won’t go into here). . Once your notebook instance is set up, you should be able to use this link to upload and run the notebook. . Install the KFP SDK . Next, we’ll install the KFP SDK. In a notebook, you may need to restart the kernel so it’s available for import. . !pip install --user -U kfp kfp-server-api . Next, we’ll do some imports: . import kfp # the Pipelines SDK. from kfp import compiler import kfp.dsl as dsl import kfp.gcp as gcp import kfp.components as comp . Defining a new ‘lightweight component’ based on a python function . ‘Lightweight’ KFP python components allow you to create a component from a python function definition, and do not require you to build a new container image for every code change. They’re helpful for fast iteration in a notebook environment. You can read more here. . In this section, we’ll create a lightweight component that uses training metrics info to decide whether to deploy a model. We’ll pass a “threshold” dict as a component arg, and compare those thresholds to the metrics values, and use that info to decide whether or not to deploy. Then we’ll output a string indicating the decision. . (As mentioned above, for production models you’d probably want to do a more substantial analysis. The TFMA library might be of interest. Stay tuned for a follow-on post that uses TFMA). . Then we’ll define a pipeline that uses the new component. In the pipeline spec, we’ll make the ‘serve’ step conditional on the “metrics” op output. . First, we’ll define the component function, eval_metrics: . from typing import NamedTuple def eval_metrics( metrics: str, thresholds: str ) -&gt; NamedTuple(&#39;Outputs&#39;, [(&#39;deploy&#39;, str)]): import json import logging def regression_threshold_check(metrics_info): for k, v in thresholds_dict.items(): logging.info(&#39;k {}, v {}&#39;.format(k, v)) if k in [&#39;root_mean_squared_error&#39;, &#39;mae&#39;]: if metrics_info[k][-1] &gt; v: logging.info(&#39;{} &gt; {}; returning False&#39;.format(metrics_info[k][0], v)) return (&#39;False&#39;, ) return (&#39;deploy&#39;, ) logging.getLogger().setLevel(logging.INFO) thresholds_dict = json.loads(thresholds) logging.info(&#39;thresholds dict: {}&#39;.format(thresholds_dict)) logging.info(&#39;metrics: %s&#39;, metrics) metrics_dict = json.loads(metrics) logging.info(&quot;got metrics info: %s&quot;, metrics_dict) res = regression_threshold_check(metrics_dict) logging.info(&#39;deploy decision: %s&#39;, res) return res . To keep things simple, we’re comparing only RMSE and MAE with given threshold values. (This function is tailored for our Keras regression model). Lower is better, so if a threshold value is higher than the associated model metric, we won’t deploy. . Next, we’ll create a ‘container op’ from the eval_metrics function definition, via the funct_to_container_op method. As one of the method args, we specify the base container image that will run the function. Here, we’re using one of the Deep Learning Container images. (This container image installs more than is necessary for this simple function, but these DL images can be useful for many ML-related components). . eval_metrics_op = comp.func_to_container_op(eval_metrics, base_image=&#39;gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest&#39;) . Define a pipeline that uses the new “metrics” op . Now, we can define a new pipeline that uses the new op and makes the model serving conditional on the results. . The new eval_metrics_op takes as an input one of the train_op outputs, which outputs a final metrics dict. (We “cheated” a bit, as the training component was already designed to output this info; in other cases you might end up defining a new version of such an op that outputs the new info you need). . Then, we’ll wrap the serving op in a conditional; we won’t set up a TF-serving service unless the eval_metrics op has certified that it is okay. . Note that this new version of the pipeline also has a new input parameter— the thresholds dict. . To keep things simple, we’ll first define a pipeline that skips the HP tuning part of the pipeline used here. This will make it easier to test your new op with a pipeline that takes a shorter time to run. . Then in a following section we’ll show how to augment the full HP tuning pipeline to include the new op. . We’ll first instantiate the other pipeline ops from their reusable components definitions. (And we’ve defined the eval_metrics_op above). . train_op = comp.load_component_from_url( &#39;https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/train_component.yaml&#39; ) serve_op = comp.load_component_from_url( &#39;https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/serve_component.yaml&#39; ) tb_op = comp.load_component_from_url( &#39;https://raw.githubusercontent.com/kubeflow/pipelines/master/components/tensorflow/tensorboard/prepare_tensorboard/component.yaml&#39; ) . Next, we’ll define the pipeline itself. You might notice that this pipeline has a new parameter, thresholds. . This pipeline first sets up a TensorBoard visualization for monitoring the training run. Then it starts the training. Once training is finished, the new op checks whether the trained model’s final metrics are above the given threshold(s). If so (using the KFP dsl.Condition construct), TF-serving is used to set up a prediction service on the Pipelines GKE cluster. . You can see that data is being passed between the pipeline ops. Here’s a tutorial that goes into how that works in more detail. . @dsl.pipeline( name=&#39;bikes_weather_metrics&#39;, description=&#39;Model bike rental duration given weather&#39; ) def bikes_weather_metrics( train_epochs: int = 2, working_dir: str = &#39;gs://YOUR/GCS/PATH&#39;, # for the full training jobs data_dir: str = &#39;gs://aju-dev-demos-codelabs/bikes_weather/&#39;, steps_per_epoch: int = -1 , # if -1, don&#39;t override normal calcs based on dataset size hptune_params: str = &#39;[{&quot;num_hidden_layers&quot;: %s, &quot;learning_rate&quot;: %s, &quot;hidden_size&quot;: %s}]&#39; % (3, 1e-2, 64), thresholds: str = &#39;{&quot;root_mean_squared_error&quot;: 2000}&#39; ): # create TensorBoard viz for the parent directory of all training runs, so that we can # compare them. tb_viz = tb_op( log_dir_uri=&#39;%s/%s&#39; % (working_dir, dsl.RUN_ID_PLACEHOLDER) ) train = train_op( data_dir=data_dir, workdir=&#39;%s/%s&#39; % (tb_viz.outputs[&#39;log_dir_uri&#39;], 0), tb_dir=tb_viz.outputs[&#39;log_dir_uri&#39;], epochs=train_epochs, steps_per_epoch=steps_per_epoch, hp_idx=0, hptune_results=hptune_params ) eval_metrics = eval_metrics_op( thresholds=thresholds, metrics=train.outputs[&#39;metrics_output_path&#39;], ) with dsl.Condition(eval_metrics.outputs[&#39;deploy&#39;] == &#39;deploy&#39;): # conditional serving serve = serve_op( model_path=train.outputs[&#39;train_output_path&#39;], model_name=&#39;bikesw&#39;, namespace=&#39;default&#39; ) train.set_gpu_limit(2) . Now we can run the pipeline from the notebook. First create a client object to talk to your KFP installation. Using that client, create (or get) an Experiment (which lets you create semantic groupings of pipeline runs). . You’ll need to set the correct host endpoint for your pipelines installation when you create the client. Visit the Pipelines panel in the Cloud Console and click on the SETTINGS gear for the desired installation to get its endpoint. . # CHANGE THIS with the info for your KFP cluster installation client = kfp.Client(host=&#39;xxxxxxxx-dot-us-centralx.pipelines.googleusercontent.com&#39;) . exp = client.create_experiment(name=&#39;bw_expers&#39;) # this is a &#39;get or create&#39; call . (If the create_experiment call failed, double check your host endpoint value). . Now, we can compile and then run the pipeline. We’ll set some vars with pipeline params: . WORKING_DIR = &#39;gs://YOUR_GCS/PATH&#39; TRAIN_EPOCHS = 2 . Now we’ll compile and run the pipeline. . Note that this pipeline is configured to use a GPU node for the training step, so make sure that you have set up a GPU node pool for the cluster that your KFP installation is running on, as described in this README. Note also that GPU nodes are more expensive. If you want, you can comment out the train.set_gpu_limit(2) line in the pipeline definition above to run training on a CPU node. . compiler.Compiler().compile(bikes_weather_metrics, &#39;bikes_weather_metrics.tar.gz&#39;) . run = client.run_pipeline(exp.id, &#39;bw_metrics_test&#39;, &#39;bikes_weather_metrics.tar.gz&#39;, params={&#39;working_dir&#39;: WORKING_DIR, &#39;train_epochs&#39;: TRAIN_EPOCHS # &#39;thresholds&#39;: THRESHOLDS }) . Once you’ve kicked off the run, click the generated link to see the pipeline run in the Kubeflow Pipelines dashboard of your pipelines installation. (See the last section for more info on how to use your trained and deployed model for prediction). . Note: It’s also possible to start a pipeline run directly from the pipeline function definition, skipping the local compilation, like this: . kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(&lt;pipeline_function_name&gt;, arguments={}) . Use the new “metrics” op with the full Keras Tuner pipeline . To keep things simple, the pipeline above didn’t do an HP tuning search. Below is how the full pipeline from this tutorial would be redefined to use this new op. . This definition assumes that you’ve run the cells above that instantiated the ops from their component specs. This new definition includes an additional hptune op (defined “inline” using dsl.ContainerOp()) that deploys the distributed HP tuning job and then waits for the results. . Important note: this example may take a long time to run, and incur significant charges in its use of GPUs, depending upon how its parameters are configured. . @dsl.pipeline( name=&#39;bikes_weather_keras_tuner&#39;, description=&#39;Model bike rental duration given weather, use Keras Tuner&#39; ) def bikes_weather_hptune( tune_epochs: int = 2, train_epochs: int = 5, num_tuners: int = 8, bucket_name: str = &#39;YOUR_BUCKET_NAME&#39;, # used for the HP dirs; don&#39;t include the &#39;gs://&#39; tuner_dir_prefix: str = &#39;hptest&#39;, tuner_proj: str = &#39;p1&#39;, max_trials: int = 128, working_dir: str = &#39;gs://YOUR/GCS/PATH&#39;, # for the full training jobs data_dir: str = &#39;gs://aju-dev-demos-codelabs/bikes_weather/&#39;, steps_per_epoch: int = -1 , # if -1, don&#39;t override normal calcs based on dataset size num_best_hps: int = 2, # the N best parameter sets for full training # the indices to the best param sets; necessary in addition to the above param because of # how KFP loops work currently. Must be consistent with the above param. num_best_hps_list: list = [0, 1], thresholds: str = &#39;{&quot;root_mean_squared_error&quot;: 2000}&#39; ): hptune = dsl.ContainerOp( name=&#39;ktune&#39;, image=&#39;gcr.io/google-samples/ml-pipeline-bikes-dep:b97ee76&#39;, arguments=[&#39;--epochs&#39;, tune_epochs, &#39;--num-tuners&#39;, num_tuners, &#39;--tuner-dir&#39;, &#39;%s/%s&#39; % (tuner_dir_prefix, dsl.RUN_ID_PLACEHOLDER), &#39;--tuner-proj&#39;, tuner_proj, &#39;--bucket-name&#39;, bucket_name, &#39;--max-trials&#39;, max_trials, &#39;--namespace&#39;, &#39;default&#39;, &#39;--num-best-hps&#39;, num_best_hps, &#39;--executions-per-trial&#39;, 2, &#39;--deploy&#39; ], file_outputs={&#39;hps&#39;: &#39;/tmp/hps.json&#39;}, ) # create TensorBoard viz for the parent directory of all training runs, so that we can # compare them. tb_viz = tb_op( log_dir_uri=&#39;%s/%s&#39; % (working_dir, dsl.RUN_ID_PLACEHOLDER) ) with dsl.ParallelFor(num_best_hps_list) as idx: # start the full training runs in parallel train = train_op( data_dir=data_dir, workdir=&#39;%s/%s&#39; % (tb_viz.outputs[&#39;log_dir_uri&#39;], idx), tb_dir=tb_viz.outputs[&#39;log_dir_uri&#39;], epochs=train_epochs, steps_per_epoch=steps_per_epoch, hp_idx=idx, hptune_results=hptune.outputs[&#39;hps&#39;] ) eval_metrics = eval_metrics_op( thresholds=thresholds, metrics=train.outputs[&#39;metrics_output_path&#39;], ) with dsl.Condition(eval_metrics.outputs[&#39;deploy&#39;] == &#39;deploy&#39;): # conditional serving serve = serve_op( model_path=train.outputs[&#39;train_output_path&#39;], model_name=&#39;bikesw&#39;, namespace=&#39;default&#39; ) train.set_gpu_limit(2) . If you want, you can compile and run this pipeline the same way as was done in the previous section. You can also find this pipeline in the example repo here. . More detail on the code, and requesting predictions from your model . This example didn’t focus on the details of the pipeline component (step) implementations. The training component uses a Keras model (TF 2.3). The serving component uses TF-serving: once the serving service is up and running, you can send prediction requests to your trained model. . You can find more detail on these components, and an example of sending a prediction request, here. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html",
            "relUrl": "/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Running a distributed Keras HP Tuning search using Kubeflow Pipelines",
            "content": "Introduction . The performance of a machine learning model is often crucially dependent on the choice of good hyperparameters. For models of any complexity, relying on trial and error to find good values for these parameters does not scale. This tutorial shows how to use Cloud AI Platform Pipelines in conjunction with the Keras Tuner libraries to build a hyperparameter-tuning workflow that runs a distributed HP search on GKE. . Cloud AI Platform Pipelines, currently in Beta, provides a way to deploy robust, repeatable machine learning pipelines along with monitoring, auditing, version tracking, and reproducibility, and gives you an easy-to-install, secure execution environment for your ML workflows. AI Platform Pipelines is based on Kubeflow Pipelines (KFP) installed on a Google Kubernetes Engine (GKE) cluster, and can run pipelines specified via both the KFP and TFX SDKs. See this blog post for more detail on the Pipelines tech stack. You can create an AI Platform Pipelines installation with just a few clicks. After installing, you access AI Platform Pipelines by visiting the AI Platform Panel in the Cloud Console. . Keras Tuner is a distributable hyperparameter optimization framework. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. It comes with several search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms. It is straightforward to run the Keras Tuner in distributed search mode, which we’ll leverage for this example. . The intent of a HP tuning search is typically not to do full training for each parameter combination, but to find the best starting points. The number of epochs run in the HP search trials are typically smaller than that used in the full training. So, an HP tuning-based ML workflow could include these steps: . perform a distributed HP tuning search, and obtain the results | do concurrent full training runs for each of the best N parameter configurations, and export (save) the model for each | serve (some of) the resultant models, often after model evaluation. | . As indicated above, a Cloud AI Platform (KFP) Pipeline runs under the hood on a GKE cluster. This makes it straightforward to implement this workflow— leveraging GKE for the distributed HP search and model serving— so that you just need to launch a pipeline job to kick it off. . This post highlights an example pipeline that does that. The example also shows how to use preemptible GPU-enabled VMS for the HP search, to reduce costs; and how to use TF-serving to deploy the trained model(s) on the same cluster for serving. As part of the process, we’ll see how GKE provides a scalable, resilient platform with easily-configured use of accelerators. . About the dataset and modeling task . The dataset . The Cloud Public Datasets Program makes available public datasets that are useful for experimenting with machine learning. Just as with this “Explaining model predictions on structured data” post, we’ll use data that is essentially a join of two public datasets stored in BigQuery: London Bike rentals and NOAA weather data, with some additional processing to clean up outliers and derive additional GIS and day-of-week fields. . The modeling task and Keras model . We’ll use this dataset to build a Keras regression model to predict the duration of a bike rental based on information about the start and end stations, the day of the week, the weather on that day, and other data. If we were running a bike rental company, for example, these predictions—and their explanations—could help us anticipate demand and even plan how to stock each location. . We’ll build a parameterizable model architecture for this task (similar in structure to a “wide and deep” model), then use the Keras Tuner package to do an HP search using this model structure, as well as doing full model training using the best HP set(s). . Keras tuner in distributed mode on GKE with preemptible VMs . With the Keras Tuner, you set up a HP tuning search along these lines (the code is from the example; other search algorithms are supported in addition to ‘random’): . tuner = RandomSearch( create_model, objective=&#39;val_mae&#39;, max_trials=args.max_trials, distribution_strategy=STRATEGY, executions_per_trial=args.executions_per_trial, directory=args.tuner_dir, project_name=args.tuner_proj ) . …where in the above, the create_model call takes takes an argument hp from which you can sample hyperparameters. For this example, we’re varying number of hidden layers, number of nodes per hidden layer, and learning rate in the HP search. There are many other hyperparameters that you might also want to vary in your search. . def create_model(hp): inputs, sparse, real = bwmodel.get_layers() ... model = bwmodel.wide_and_deep_classifier( inputs, linear_feature_columns=sparse.values(), dnn_feature_columns=real.values(), num_hidden_layers=hp.Int(&#39;num_hidden_layers&#39;, 2, 5), dnn_hidden_units1=hp.Int(&#39;hidden_size&#39;, 32, 256, step=32), learning_rate=hp.Choice(&#39;learning_rate&#39;, values=[1e-1, 1e-2, 1e-3, 1e-4]) ) . Then, call tuner.search(...). See the Keras Tuner docs for more. . The Keras Tuner supports running this search in distributed mode. Google Kubernetes Engine (GKE) makes it straightforward to configure and run a distributed HP tuning search. GKE is a good fit not only because it lets you easily distribute the HP tuning workload, but because you can leverage autoscaling to boost node pools for a large job, then scale down when the resources are no longer needed. It’s also easy to deploy trained models for serving onto the same GKE cluster, using TF-serving. In addition, the Keras Tuner works well with preemptible VMs, making it even cheaper to run your workloads. . With the Keras Tuner’s distributed config, you specify one node as the ‘chief’, which coordinates the search, and ‘tuner’ nodes that do the actual work of running model training jobs using a given param set (the trials). When you set up an HP search, you indicate the max number of trials to run, and how many executions to run per trial. The Kubeflow Pipeline allows dynamic specification of the number of tuners to use for a given HP search— this determines how many trials you can run concurrently— as well as the max number of trials and number of executions. . We’ll define the tuner components as Kubernetes jobs, each specified to have 1 replica. This means that if a tuner job pod is terminated for some reason prior to job completion, Kubernetes will start up another replica. Thus, the Keras Tuner’s HP search is a good fit for use of preemptible VMs. Because the HP search bookkeeping— orchestrated by the tuner chief, via an ‘oracle’ file— tracks the state of the trials, the configuration is robust to a tuner pod terminating unexpectedly— say, due to a preemption— and a new one being restarted. The new job pod will get its instructions from the ‘oracle’ and continue running trials. The example uses GCS for the tuners’ shared file system. . Once the HP search has finished, any of the tuners can obtain information on the N best parameter sets (as well as export the best model(s)). . Defining the HP Tuning + training workflow as a pipeline . The definition of the pipeline itself is here, specified using the KFP SDK. It’s then compiled to an archive file and uploaded to AI Platforms Pipelines. (To compile it yourself, you’ll need to have the KFP SDK installed). Pipeline steps are container-based, and you can find the Dockerfiles and underlying code for the steps under the example’s components directory. . The example pipeline first runs a distributed HP tuning search using a specified number of tuner workers, then obtains the best N parameter sets—by default, it grabs the best two. The pipeline step itself does not do the heavy lifting, but rather launches all the tuner jobs on GKE, which run concurrently, and monitors for their completion. (Unsurprisingly, this stage of the pipeline may run for quite a long time, depending upon how many HP search trials were specified and how many tuners are used for the distributed search). . Concurrently to the Keras Tuner runs, the pipeline sets up a TensorBoard visualization component, its log directory set to the GCS path under which the full training jobs are run. . The pipeline then runs full training jobs, concurrently, for each of the N best parameter sets. It does this via the KFP loop construct, allowing the pipeline to support dynamic specification of N. The training jobs can be monitored and compared using TensorBoard, both while they’re running and after they’ve completed. . Then, the trained models are deployed for serving for serving on the GKE cluster, using TF-serving. Each deployed model has its own cluster service endpoint. (While not included in this example, one could insert a step for model evaluation before making the decision about whether to deploy to TF-serving.) . For example, here is the DAG for a pipeline execution that did training and then deployed prediction services for the two best parameter configurations. . The DAG for keras tuner pipeline execution. Here the two best parameter configurations were used for full training. Running the example pipeline . Note: this example may take a long time to run, and incur significant charges in its use of GPUs, depending upon how its parameters are configured. . To run the example yourself, and for more detail on the KFP pipeline’s components, see the example’s README. . What’s next? . One obvious next step in development of the workflow would be to add components that evaluate each full model after training, before determining whether to deploy it. One approach could be to use TensorFlow Model Analysis (TFMA). Stay tuned for some follow-on posts that explore how to do that using KFP. (Update: one such post is here). . While not covered in this example, it would alternatively have been straightforward to deploy and serve the trained model(s) using AI Platform Prediction. . Another alternative would be to use AI Platform Vizier for hyperparameter tuning search instead of the Keras Tuner libraries. Stay tuned for a post on that as well. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/kfp/kubeflow/keras/tensorflow/hp_tuning/2020/10/19/keras_tuner.html",
            "relUrl": "/ml/kfp/kubeflow/keras/tensorflow/hp_tuning/2020/10/19/keras_tuner.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Training an AutoML Tables model with BigQuery ML",
            "content": "Introduction . BigQuery ML (BQML) enables users to create and execute machine learning models in BigQuery by using SQL queries. . AutoML Tables lets you automatically build, analyze, and deploy state-of-the-art machine learning models using your own structured data, and explain prediction results. It’s useful for a wide range of machine learning tasks, such as asset valuations, fraud detection, credit risk analysis, customer retention prediction, analyzing item layouts in stores, solving comment section spam problems, quickly categorizing audio content, predicting rental demand, and more. (This blog post gives more detail on many of its capabilities). . Recently, BQML added support for AutoML Tables models (in Beta). This makes it easy to train Tables models on your BigQuery data using standard SQL, directly from the BigQuery UI (or API), and to evaluate and use the models for prediction directly via SQL as well. . In this post, we’ll take a look at how to do this, and show a few tips as well. . About the dataset and modeling task . The Cloud Public Datasets Program makes available public datasets that are useful for experimenting with machine learning. We’ll use data that is essentially a join of two public datasets stored in BigQuery: London Bike rentals and NOAA weather data, with some additional processing to clean up outliers and derive additional GIS and day-of-week fields. The table we’ll use is here: aju-dev-demos.london_bikes_weather.bikes_weather. . Using this dataset, we’ll build a regression model to predict the duration of a bike rental based on information about the start and end rental stations, the day of the week, the weather on that day, and other data. (If we were running a bike rental company, we could use these predictions—and their explanations—to help us anticipate demand and even plan how to stock each location). . Specifying training, eval, and test datasets . AutoML Tables will split the data you send it into its own training/test/validation sets. . Note: it’s also possible to specify the data split column as a BQML model creation option: DATA_SPLIT_COL = string_value. string_value is one of the columns in the training data and should be either a timestamp or string column. See the documentation for more detail. . For BQML, we’ll split the data into a set to use for the training process, and reserve a ‘test’ dataset that AutoML training never sees. We don’t want to just grab a sequential slice of the table for each. There’s an easy way to accomplish this in a repeatable manner by using the Farm Hash algorithm, implemented as the FARM_FINGERPRINT BigQuery SQL function. We’d like to create a 90/10 split. . So, the query to generate training data will include a clause like this: . WHERE ABS(MOD(FARM_FINGERPRINT(timestamp), 10)) &lt; 9 . Similarly, the query to generate the ‘test’ set will use this clause: . WHERE ABS(MOD(FARM_FINGERPRINT(timestamp), 10)) = 9 . Using this approach, we can build SELECT clauses with reproducible results that give us datasets with the split proportions we want. . Tables schema configuration and BQML . If you’ve used AutoML Tables, you may have noticed that after a dataset is ingested, it’s possible to adjust the inferred field (column) schema information. For example, you might have some fields with numeric values that you’d like to treat as categorical when you train your custom model. This is the case for our dataset, where we’d like to treat the numeric rental station IDs as categorical. . With BQML, it’s not currently possible to explicitly specify the schema for the model inputs, but for numerics that we want to treat as categorical, we can provide a strong hint by casting them to strings; then Tables will decide whether to treat such values as ‘text’ or ‘categorical’. So, in the SQL below, you’ll see that the day_of_week, start_station_id, and end_station_id columns are all cast to strings. . Training the AutoML Tables model via BQML . To train the Tables model, we’ll pick the AUTOML_REGRESSOR model type (since we want to predict duration, a numeric value. In the query, we’ll specify duration in the input_label_cols list (thus indicating the “label” column”, and set budget_hours to 1, meaning that we’re budgeting one hour of training time. (The training process, which includes setup and teardown, etc., will typically take longer). . Here’s the resultant BigQuery query (to run it yourself, substitute your project id, dataset, and model name in the first line, then paste the query into the BigQuery UI query window in the Cloud Console): . CREATE OR REPLACE MODEL `YOUR-PROJECT-ID.YOUR-DATASET.YOUR-MODEL-NAME`        OPTIONS(model_type=&#39;AUTOML_REGRESSOR&#39;,                input_label_cols=[&#39;duration&#39;],                budget_hours=1.0) AS SELECT   duration, ts, cast(day_of_week as STRING) as dow, start_latitude, start_longitude, end_latitude, end_longitude, euclidean, loc_cross, prcp, max, min, temp, dewp, cast(start_station_id as STRING) as ssid, cast(end_station_id as STRING) as esid FROM `aju-dev-demos.london_bikes_weather.bikes_weather` WHERE ABS(MOD(FARM_FINGERPRINT(cast(ts as STRING)), 10)) &lt; 9 . Note the casts to STRING and the use of FARM_FINGERPRINT as discussed above. . (If you’ve taken a look at the bikes_weather table, you might notice that the SELECT clause does not include the bike_id column. A previously-run AutoML Tables analysis of the global feature importance of the dataset fields indicated that bike_id had negligible impact, so we won’t use it for this model). . Global feature importance rankings. Evaluating your trained custom model . After the training has completed, you can view the evaluation metrics for your custom model, and also run your own evaluation query yourself. You can view the evaluation metrics generated during the training process by clicking on the model name in the BigQuery UI, then click on the “Evaluation” tab in the central panel. It will look something like this: . Model evaluation metrics generated during the training process (At time of writing, some of this data is incomplete, but that will change soon). . The BigQuery SQL to run your own evaluation query for the trained model looks like this (again, substitute your own project, dataset, and model name): . SELECT   * FROM   ML.EVALUATE(MODEL `YOUR-PROJECT-ID.YOUR-DATASET.YOUR-MODEL-NAME`, ( SELECT   duration, ts, cast(day_of_week as STRING) as dow, start_latitude, start_longitude, end_latitude, end_longitude, euclidean, loc_cross, prcp, max, min, temp, dewp, cast(start_station_id as STRING) as ssid, cast(end_station_id as STRING) as esid FROM   `aju-dev-demos.london_bikes_weather.bikes_weather` WHERE ABS(MOD(FARM_FINGERPRINT(cast(ts as STRING)), 10)) = 9 )) . Note that via the FARM_FINGERPRINT function, we’re using a different dataset for evaluation than we used for training. The evaluation results should look something like the following. The metrics will be a bit different from those above, since we’re using different data than AutoML Tables used for its eval split. . Model evaluation results. Did our schema hints help? . It’s interesting to check whether the schema hints (casting some of the numeric fields to strings) made a difference in model accuracy. To try this yourself, create another differently-named model as shown in the training section above, but for the SELECT clause, don’t include the casts to STRING, e.g.: . ...AS SELECT   duration, ts, day_of_week, start_latitude, start_longitude, end_latitude, end_longitude, euclidean, loc_cross, prcp, max, min, temp, dewp, start_station_id, end_station_id . Then, evaluate this second model (again substituting the details for your own project): . SELECT   * FROM   ML.EVALUATE(MODEL `YOUR-PROJECT-ID.YOUR-DATASET.YOUR-MODEL-NAME2`, ( SELECT   duration, ts, day_of_week, start_latitude, start_longitude, end_latitude, end_longitude, euclidean, loc_cross, prcp, max, min, temp, dewp, start_station_id, end_station_id FROM   `aju-dev-demos.london_bikes_weather.bikes_weather` WHERE ABS(MOD(FARM_FINGERPRINT(cast(ts as STRING)), 10)) = 9 )) . When I evaluated this second model, which kept the station IDs and day of week as numerics, the results showed that this model was somewhat less accurate: . Evaluation results for the model that did not cast categorical numerics to strings. Using your BQML AutoML Tables model for prediction . Once your model is trained, and you’ve ascertained it’s accurate enough, you can use it for prediction. Here’s an example of how to do that. Via the FARM_FINGERPRINT function, we’re drawing from our “test” split, but because the resultant dataset is large, we’re just grabbing a few rows (200) for the query below: . SELECT * FROM ML.PREDICT(MODEL `YOUR-PROJECT-ID.YOUR-DATASET.YOUR-MODEL-NAME`, (SELECT   duration, ts, cast(day_of_week as STRING) as dow, start_latitude, start_longitude, end_latitude, end_longitude, euclidean, loc_cross, prcp, max, min, temp, dewp, cast(start_station_id as STRING) as ssid, cast(end_station_id as STRING) as esid FROM `aju-dev-demos.london_bikes_weather.bikes_weather` WHERE ABS(MOD(FARM_FINGERPRINT(cast(ts as STRING)), 10)) = 9)) limit 200 . The prediction results will look something like this (click to see larger version): . Model prediction results. Note that while the query above is “standalone”, you can of course access model prediction results as part of a larger BigQuery query too. . Summary . In this post, we showed how to train an AutoML Tables model using BQML, evaluate the model, and then use it for prediction— all from BigQuery. The BQML documentation has more detail on getting started, and resources for the other model types available through BQML. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/automl/bigquery/2020/07/14/bqml_tables.html",
            "relUrl": "/ml/automl/bigquery/2020/07/14/bqml_tables.html",
            "date": " • Jul 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Using Google Cloud Functions to support event-based triggering of Cloud AI Platform Pipelines",
            "content": "This example shows how you can run a Cloud AI Platform Pipeline from a Google Cloud Function, thus providing a way for Pipeline runs to be triggered by events (in the interim before this is supported by Pipelines itself). . In this example, the function is triggered by the addition of or update to a file in a Google Cloud Storage (GCS) bucket, but Cloud Functions can have other triggers too (including Pub/Sub-based triggers). . The example is Google Cloud Platform (GCP)-specific, and requires a Cloud AI Platform Pipelines installation using Pipelines version &gt;= 0.4. To run this example as a notebook, click on one of the badges at the top of the page or see here. . (If you are instead interested in how to do this with a Kubeflow-based pipelines installation, see this notebook). . Setup . Create a Cloud AI Platform Pipelines installation . Follow the instructions in the documentation to create a Cloud AI Platform Pipelines installation. . Identify (or create) a Cloud Storage bucket to use for the example . Before executing the next cell, edit it to set the TRIGGER_BUCKET environment variable to a Google Cloud Storage bucket (create a bucket first if necessary). Do not include the gs:// prefix in the bucket name. . We&#39;ll deploy the GCF function so that it will trigger on new and updated files (blobs) in this bucket. . %env TRIGGER_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET_NAME . Give Cloud Function&#39;s service account the necessary access . First, make sure the Cloud Function API is enabled. . Cloud Functions uses the project&#39;s &#39;appspot&#39; acccount for its service account. It will have the form: PROJECT_ID@appspot.gserviceaccount.com. (This is also the project&#39;s App Engine service account). . Go to your project&#39;s IAM - Service Account page. | Find the PROJECT_ID@appspot.gserviceaccount.com account and copy its email address. | Find the project&#39;s Compute Engine (GCE) default service account (this is the default account used for the Pipelines installation). It will have a form like this: PROJECT_NUMBER@developer.gserviceaccount.com. Click the checkbox next to the GCE service account, and in the &#39;INFO PANEL&#39; to the right, click ADD MEMBER. Add the Functions service account (PROJECT_ID@appspot.gserviceaccount.com) as a Project Viewer of the GCE service account. | . . Next, configure your TRIGGER_BUCKET to allow the Functions service account access to that bucket. . Navigate in the console to your list of buckets in the Storage Browser. | Click the checkbox next to the TRIGGER_BUCKET. In the &#39;INFO PANEL&#39; to the right, click ADD MEMBER. Add the service account (PROJECT_ID@appspot.gserviceaccount.com) with Storage Object Admin permissions. (While not tested, giving both Object view and create permissions should also suffice). | . . Create a simple GCF function to test your configuration . First we&#39;ll generate and deploy a simple GCF function, to test that the basics are properly configured. . %%bash mkdir -p functions . We&#39;ll first create a requirements.txt file, to indicate what packages the GCF code requires to be installed. (We won&#39;t actually need kfp for this first &#39;sanity check&#39; version of a GCF function, but we&#39;ll need it below for the second function we&#39;ll create, that deploys a pipeline). . %%writefile functions/requirements.txt kfp . Next, we&#39;ll create a simple GCF function in the functions/main.py file: . %%writefile functions/main.py import logging def gcs_test(data, context): &quot;&quot;&quot;Background Cloud Function to be triggered by Cloud Storage. This generic function logs relevant data when a file is changed. Args: data (dict): The Cloud Functions event payload. context (google.cloud.functions.Context): Metadata of triggering event. Returns: None; the output is written to Stackdriver Logging &quot;&quot;&quot; logging.info(&#39;Event ID: {}&#39;.format(context.event_id)) logging.info(&#39;Event type: {}&#39;.format(context.event_type)) logging.info(&#39;Data: {}&#39;.format(data)) logging.info(&#39;Bucket: {}&#39;.format(data[&#39;bucket&#39;])) logging.info(&#39;File: {}&#39;.format(data[&#39;name&#39;])) file_uri = &#39;gs://%s/%s&#39; % (data[&#39;bucket&#39;], data[&#39;name&#39;]) logging.info(&#39;Using file uri: %s&#39;, file_uri) logging.info(&#39;Metageneration: {}&#39;.format(data[&#39;metageneration&#39;])) logging.info(&#39;Created: {}&#39;.format(data[&#39;timeCreated&#39;])) logging.info(&#39;Updated: {}&#39;.format(data[&#39;updated&#39;])) . Deploy the GCF function as follows. (You&#39;ll need to wait a moment or two for output of the deployment to display in the notebook). You can also run this command from a notebook terminal window in the functions subdirectory. . %%bash cd functions gcloud functions deploy gcs_test --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize . After you&#39;ve deployed, test your deployment by adding a file to the specified TRIGGER_BUCKET. You can do this easily by visiting the Storage panel in the Cloud Console, clicking on the bucket in the list, and then clicking on Upload files in the bucket details view. . Then, check in the logs viewer panel (https://console.cloud.google.com/logs/viewer) to confirm that the GCF function was triggered and ran correctly. You can select &#39;Cloud Function&#39; in the first pulldown menu to filter on just those log entries. . Deploy a Pipeline from a GCF function . Next, we&#39;ll create a GCF function that deploys an AI Platform Pipeline when triggered. First, preserve your existing main.py in a backup file: . %%bash cd functions mv main.py main.py.bak . Then, before executing the next cell, edit the HOST variable in the code below. You&#39;ll replace &lt;your_endpoint&gt; with the correct value for your installation. . To find this URL, visit the Pipelines panel in the Cloud Console. From here, you can find the URL by clicking on the SETTINGS link for the Pipelines installation you want to use, and copying the &#39;host&#39; string displayed in the client example code (prepend https:// to that string in the code below). You can alternately click on OPEN PIPELINES DASHBOARD for the Pipelines installation, and copy that URL, removing the /#/pipelines suffix. . %%writefile functions/main.py import logging import datetime import logging import time import kfp import kfp.compiler as compiler import kfp.dsl as dsl import requests # TODO: replace with your Pipelines endpoint URL HOST = &#39;https://&lt;your_endpoint&gt;.pipelines.googleusercontent.com&#39; @dsl.pipeline( name=&#39;Sequential&#39;, description=&#39;A pipeline with two sequential steps.&#39; ) def sequential_pipeline(filename=&#39;gs://ml-pipeline-playground/shakespeare1.txt&#39;): &quot;&quot;&quot;A pipeline with two sequential steps.&quot;&quot;&quot; op1 = dsl.ContainerOp( name=&#39;filechange&#39;, image=&#39;library/bash:4.4.23&#39;, command=[&#39;sh&#39;, &#39;-c&#39;], arguments=[&#39;echo &quot;%s&quot; &gt; /tmp/results.txt&#39; % filename], file_outputs={&#39;newfile&#39;: &#39;/tmp/results.txt&#39;}) op2 = dsl.ContainerOp( name=&#39;echo&#39;, image=&#39;library/bash:4.4.23&#39;, command=[&#39;sh&#39;, &#39;-c&#39;], arguments=[&#39;echo &quot;%s&quot;&#39; % op1.outputs[&#39;newfile&#39;]] ) def get_access_token(): url = &#39;http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token&#39; r = requests.get(url, headers={&#39;Metadata-Flavor&#39;: &#39;Google&#39;}) r.raise_for_status() access_token = r.json()[&#39;access_token&#39;] return access_token def hosted_kfp_test(data, context): logging.info(&#39;Event ID: {}&#39;.format(context.event_id)) logging.info(&#39;Event type: {}&#39;.format(context.event_type)) logging.info(&#39;Data: {}&#39;.format(data)) logging.info(&#39;Bucket: {}&#39;.format(data[&#39;bucket&#39;])) logging.info(&#39;File: {}&#39;.format(data[&#39;name&#39;])) file_uri = &#39;gs://%s/%s&#39; % (data[&#39;bucket&#39;], data[&#39;name&#39;]) logging.info(&#39;Using file uri: %s&#39;, file_uri) logging.info(&#39;Metageneration: {}&#39;.format(data[&#39;metageneration&#39;])) logging.info(&#39;Created: {}&#39;.format(data[&#39;timeCreated&#39;])) logging.info(&#39;Updated: {}&#39;.format(data[&#39;updated&#39;])) token = get_access_token() logging.info(&#39;attempting to launch pipeline run.&#39;) ts = int(datetime.datetime.utcnow().timestamp() * 100000) client = kfp.Client(host=HOST, existing_token=token) compiler.Compiler().compile(sequential_pipeline, &#39;/tmp/sequential.tar.gz&#39;) exp = client.create_experiment(name=&#39;gcstriggered&#39;) # this is a &#39;get or create&#39; op res = client.run_pipeline(exp.id, &#39;sequential_&#39; + str(ts), &#39;/tmp/sequential.tar.gz&#39;, params={&#39;filename&#39;: file_uri}) logging.info(res) . Next, deploy the new GCF function. As before, it will take a moment or two for the results of the deployment to display in the notebook. . %%bash cd functions gcloud functions deploy hosted_kfp_test --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize . Add another file to your TRIGGER_BUCKET. This time you should see both GCF functions triggered. The hosted_kfp_test function will deploy the pipeline. You&#39;ll be able to see it running at your Pipeline installation&#39;s endpoint, https://&lt;your_endpoint&gt;.pipelines.googleusercontent.com/#/pipelines, under the given Pipelines Experiment (gcstriggered as default). . . Copyright 2020, Google, LLC. Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at . http://www.apache.org/licenses/LICENSE-2.0 . Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/pipelines/mlops/kfp/gcf/2020/05/12/hosted_kfp_gcf.html",
            "relUrl": "/ml/pipelines/mlops/kfp/gcf/2020/05/12/hosted_kfp_gcf.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Creating an AutoML Tables end-to-end workflow on Cloud AI Platform Pipelines",
            "content": "Introduction . AutoML Tables lets you automatically build, analyze, and deploy state-of-the-art machine learning models using your own structured data. . A number of new AutoML Tables features have been released recently. These include: . An improved Python client library | The ability to obtain explanations for your online predictions | The ability to export your model and serve it in a container anywhere | The ability to view model search progress and final model hyperparameters in Cloud Logging | . This post gives a tour of some of these new features via a Cloud AI Platform Pipelines example, that shows end-to-end management of an AutoML Tables workflow. . The example pipeline creates a dataset, imports data into the dataset from a BigQuery view, and trains a custom model on that data. Then, it fetches evaluation and metrics information about the trained model, and based on specified criteria about model quality, uses that information to automatically determine whether to deploy the model for online prediction. Once the model is deployed, you can make prediction requests, and optionally obtain prediction explanations as well as the prediction result. In addition, the example shows how to scalably serve your exported trained model from your Cloud AI Platform Pipelines installation for prediction requests. . You can manage all the parts of this workflow from the Tables UI as well, or programmatically via a notebook or script. But specifying this process as a workflow has some advantages: the workflow becomes reliable and repeatable, and Pipelines makes it easy to monitor the results and schedule recurring runs. For example, if your dataset is updated regularly—say once a day— you could schedule a workflow to run daily, each day building a model that trains on an updated dataset. (With a bit more work, you could also set up event-based triggering pipeline runs, for example when new data is added to a Google Cloud Storage bucket.) . About the example dataset and scenario . The Cloud Public Datasets Program makes available public datasets that are useful for experimenting with machine learning. For our examples, we’ll use data that is essentially a join of two public datasets stored in BigQuery: London Bike rentals and NOAA weather data, with some additional processing to clean up outliers and derive additional GIS and day-of-week fields. Using this dataset, we’ll build a regression model to predict the duration of a bike rental based on information about the start and end rental stations, the day of the week, the weather on that day, and other data. If we were running a bike rental company, we could use these predictions—and their explanations—to help us anticipate demand and even plan how to stock each location. While we’re using bike and weather data here, you can use AutoML Tables for tasks as varied as asset valuations, fraud detection, credit risk analysis, customer retention prediction, analyzing item layouts in stores, and many more. . Using Cloud AI Platform Pipelines or Kubeflow Pipelines to orchestrate a Tables workflow . You can run this example via a Cloud AI Platform Pipelines installation, or via Kubeflow Pipelines on a Kubeflow on GKE installation. Cloud AI Platform Pipelines was recently launched in Beta. Slightly different variants of the pipeline specification are required depending upon which you’re using. (It would be possible to run the example on other Kubeflow installations too, but that would require additional credentials setup not covered in this tutorial). . Install a Cloud AI Platform Pipelines cluster . You can create an AI Platform Pipelines installation with a few clicks. Access AI Platform Pipelines by visiting the AI Platform Panel in the Cloud Console. . Create a new Pipelines instance. See the documentation for more detail. . (You can also do this installation from the command line onto an existing GKE cluster if you prefer. If you do, for consistency with the UI installation, create the GKE cluster with --scopes cloud-platform). . Or, install Kubeflow to use Kubeflow Pipelines . You can also run this example from a Kubeflow installation. For the example to work out of the box, you’ll need a Kubeflow on GKE installation, set up to use IAP. An easy way to do this is via the Kubeflow ‘click to deploy’ web app, or you can follow the command-line instructions here. . Upload and run the Tables end-to-end Pipeline . Once a Pipelines installation is running, we can upload the example AutoML Tables pipeline. Click on Pipelines in the left nav bar of the Pipelines Dashboard. Click on Upload Pipeline. . For Cloud AI Platform Pipelines, upload tables_pipeline_caip.py.tar.gz. This archive points to the compiled version of this pipeline, specified and compiled using the Kubeflow Pipelines SDK. | For Kubeflow Pipelines on a Kubeflow installation, upload tables_pipeline_kf.py.tar.gz. This archive points to the compiled version of this pipeline. To run this example on a KF installation, you will need to give the &lt;deployment-name&gt;-user@&lt;project-id&gt;.iam.gserviceaccount.com service account AutoML Admin privileges. | . Note: The difference between the two pipelines relates to how GCP authentication is handled. For the Kubeflow pipeline, we’ve added .apply(gcp.use_gcp_secret(&#39;user-gcp-sa&#39;)) annotations to the pipeline steps. This tells the pipeline to use the mounted secret—set up during the installation process— that provides GCP account credentials. With the Cloud AI Platform Pipelines installation, the GKE cluster nodes have been set up to use the cloud-platform scope. With recent Kubeflow releases, use of the mounted secret is no longer necessary, but we include both versions for compatibility. . The uploaded pipeline graph will look similar to this: . The uploaded Tables &#39;end-to-end&#39; pipeline. Click the +Create Run button to run the pipeline. You will need to fill in some pipeline parameters. Specifically, replace YOUR_PROJECT_HERE with the name of your project; replace YOUR_DATASET_NAME with the name you want to give your new dataset (make it unique, and use letters, numbers and underscores up to 32 characters); and replace YOUR_BUCKET_NAME with the name of a GCS bucket. Do not include the gs:// prefix— just enter the name. This bucket should be in the same region as that specified by the gcp_region parameter. E.g., if you keep the default us-central1 region, your bucket should also be a regional (not multi-regional) bucket in the us-central1 region. ++double check that this is necessary.++ . If you want to schedule a recurrent set of runs, you can do that instead. If your data is in BigQuery— as is the case for this example pipeline— and has a temporal aspect, you could define a view to reflect that, e.g. to return data from a window over the last N days or hours. Then, the AutoML pipeline could specify ingestion of data from that view, grabbing an updated data window each time the pipeline is run, and building a new model based on that updated window. . The steps executed by the pipeline . The example pipeline creates a dataset, imports data into the dataset from a BigQuery view, and trains a custom model on that data. Then, it fetches evaluation and metrics information about the trained model, and based on specified criteria about model quality, uses that information to automatically determine whether to deploy the model for online prediction. We’ll take a closer look at each of the pipeline steps, and how they’re implemented. . Create a Tables dataset and adjust its schema . This pipeline creates a new Tables dataset, and ingests data from a BigQuery table for the “bikes and weather” dataset described above. These actions are implemented by the first two steps in the pipeline (the automl-create-dataset-for-tables and automl-import-data-for-tables steps). . While we’re not showing it in this example, AutoML Tables supports ingestion from BigQuery views as well as tables. This can be an easy way to do feature engineering: leverage BigQuery’s rich set of functions and operators to clean and transform your data before you ingest it. . When the data is ingested, AutoML Tables infers the data type for each field (column). In some cases, those inferred types may not be what you want. For example, for our “bikes and weather” dataset, several ID fields (like the rental station IDs) are set by default to be numeric, but we want them treated as categorical when we train our model. In addition, we want to treat the loc_cross strings as categorical rather than text. . We make these adjustments programmatically, by defining a pipeline parameter that specifies the schema changes we want to make. Then, in the automl-set-dataset-schema pipeline step, for each indicated schema adjustment, we call update_column_spec: . client.update_column_spec( dataset_display_name=dataset_display_name, column_spec_display_name=column_spec_display_name, type_code=type_code, nullable=nullable ) . Before we can train the model, we also need to specify the target column— what we want our model to predict. In this case, we’ll train the model to predict rental duration. This is a numeric value, so we’ll be training a regression model. . client.set_target_column( dataset_display_name=dataset_display_name, column_spec_display_name=target_column_spec_name ) . Train a custom model on the dataset . Once the dataset is defined and its schema set properly, the pipeline will train the model. This happens in the automl-create-model-for-tables pipeline step. Via pipeline parameters, we can specify the training budget, the optimization objective (if not using the default), and can additionally specify which columns to include or exclude from the model inputs. . You may want to specify a non-default optimization objective depending upon the characteristics of your dataset. This table describes the available optimization objectives and when you might want to use them. For example, if you were training a classification model using an imbalanced dataset, you might want to specify use of AUC PR (MAXIMIZE_AU_PRC), which optimizes results for predictions for the less common class. . client.create_model( model_display_name, train_budget_milli_node_hours=train_budget_milli_node_hours, dataset_display_name=dataset_display_name, optimization_objective=optimization_objective, include_column_spec_names=include_column_spec_names, exclude_column_spec_names=exclude_column_spec_names, ) . View model search information via Cloud Logging . You can view details about an AutoML Tables model via Cloud Logging. Using Logging, you can see the final model hyperparameters as well as the hyperparameters and object values used during model training and tuning. . An easy way to access these logs is to go to the AutoML Tables page in the Cloud Console. Select the Models tab in the left navigation pane and click on the model you’re interested in. Click the “Model” link to see the final hyperparameter logs. To see the tuning trial hyperparameters, click the “Trials” link. . View a model&#39;s search logs from its evaluation information. For example, here is a look at the Trials logs a custom model trained on the “bikes and weather” dataset, with one of the entries expanded in the logs: . The &#39;Trials&#39; logs for a &quot;bikes and weather&quot; model Custom model evaluation . Once your custom model has finished training, the pipeline moves on to its next step: model evaluation. We can access evaluation metrics via the API. We’ll use this information to decide whether or not to deploy the model. . These actions are factored into two steps. The process of fetching the evaluation information can be a general-purpose component (pipeline step) used in many situations; and then we’ll follow that with a more special-purpose step, that analyzes that information and uses it to decide whether or not to deploy the trained model. . In the first of these pipeline steps— the automl-eval-tables-model step— we’ll retrieve the evaluation and global feature importance information. . model = client.get_model(model_display_name=model_display_name) feat_list = [(column.feature_importance, column.column_display_name) for column in model.tables_model_metadata.tables_model_column_info] evals = list(client.list_model_evaluations(model_display_name=model_display_name)) . AutoML Tables automatically computes global feature importance for a trained model. This shows, across the evaluation set, the average absolute attribution each feature receives. Higher values mean the feature generally has greater influence on the model’s predictions. This information is useful for debugging and improving your model. If a feature’s contribution is negligible—if it has a low value—you can simplify the model by excluding it from future training. The pipeline step renders the global feature importance data as part of the pipeline run’s output: . Global feature importance for the model inputs, rendered by a Kubeflow Pipeline step. For our example, based on the graphic above, we might try training a model without including bike_id. . In the following pipeline step— the automl-eval-metrics step— the evaluation output from the previous step is grabbed as input, and parsed to extract metrics that we’ll use in conjunction with pipeline parameters to decide whether or not to deploy the model. Note that this component is more special-purpose: unlike the other components in this pipeline, which support generalizable operations, this component— while it is parameterized— is specific in how it analyzes the evaluation info and decides whether or not to do the deployment. . One of the pipeline input parameters allows specification of metric thresholds. In this example, we’re training a regression model, and we’re specifying a mean_absolute_error (MAE) value as a threshold in the pipeline input parameters: . {&quot;mean_absolute_error&quot;: 450} . The automl-eval-metrics pipeline step compares the model evaluation information to the given threshold constraints. In this case, if the MAE is &lt; 450, the model will not be deployed. The pipeline step outputs that decision, and displays the evaluation information it’s using as part of the pipeline run’s output: . Information about a model&#39;s evaluation, rendered by a Kubeflow Pipeline step. (Conditional) model deployment . You can deploy any of your custom Tables models to make them accessible for online prediction requests. The pipeline code uses a conditional test to determine whether or not to run the step that deploys the model, based on the output of the evaluation step described above: . with dsl.Condition(eval_metrics.outputs[&#39;deploy&#39;] == True): deploy_model = deploy_model_op( ... ) . Only if the model meets the given criteria, will the deployment step (called automl-deploy-tables-model) be run, and the model be deployed automatically as part of the pipeline run: . response = client.deploy_model(model_display_name=model_display_name) . You can always deploy a model later if you like. . Putting it together: The full pipeline execution . The figure below shows the result of a pipeline run. In this case, the conditional step was executed— based on the model evaluation metrics— and the trained model was deployed. Via the UI, you can view outputs, logs for each step, run artifacts and lineage information, and more. See this post for more detail. . ++TODO: replace the following figure with something better++ . Execution of a pipeline run. You can view outputs, logs for each step, run artifacts and lineage information, and more. Getting explanations about your model’s predictions . Once a model is deployed, you can request predictions from that model. You can additionally request explanations for local feature importance: a score showing how much (and in which direction) each feature influenced the prediction for a single example. See this blog post for more information on how those values are calculated. . Here is a notebook example of how to request a prediction and its explanation using the Python client libraries. . from google.cloud import automl_v1beta1 as automl client = automl.TablesClient(project=PROJECT_ID, region=REGION) response = client.predict( model_display_name=model_display_name, inputs=inputs, feature_importance=True, ) . The prediction response will have a structure like this. (The notebook above shows how to visualize the local feature importance results using matplotlib.) . It’s easy to explore local feature importance through the Cloud Console’s AutoML Tables UI as well. After you deploy a model, go to the TEST &amp; USE tab of the Tables panel, select ONLINE PREDICTION, enter the field values for the prediction, and then check the Generate feature importance box at the bottom of the page. The result will show the feature importance values as well as the prediction. This blog post gives some examples of how these explanations can be used to find potential issues with your data or help you better understand your problem domain. . The AutoML Tables UI in the Cloud Console . With this example we’ve focused on how you can automate a Tables workflow using Kubeflow pipelines and the Python client libraries. . All of the pipeline steps can also be accomplished via the AutoML Tables UI in the Cloud Console, including many useful visualizations, and other functionality not implemented by this example pipeline— such as the ability to export the model’s test set and prediction results to BigQuery for further analysis. . Export the trained model and serve it on a GKE cluster . Recently, Tables launched a feature to let you export your full custom model, packaged so that you can serve it via a Docker container. (Under the hood, it is using TensorFlow Serving). This lets you serve your models anywhere that you can run a container, including a GKE cluster. This means that you can run a model serving service on your AI Platform Pipelines or Kubeflow installation, both of which run on GKE. . This blog post walks through the steps to serve the exported model (in this case, using Cloud Run). Follow the instructions in the post through the “View information about your exported model in TensorBoard” section. Here, we’ll diverge from the rest of the post and create a GKE service instead. . Make a copy of deploy_model_for_tables/model_serve_template.yaml file and name it model_serve.yaml. Edit this new file, replacing MODEL_NAME with some meaningful name for your model, IMAGE_NAME with the name of the container image you built (as described in the blog post, and NAMESPACE with the namespace in which you want to run your service (e.g. default). . Then, from the command line, run: . kubectl apply -f model_serve.yaml . to set up your model serving service and its underlying deployment. (Before you do that, make sure that kubectl is set to use your GKE cluster’s credentials. One way to do that is to visit the GKE panel in the Cloud Console, and click Connect for that cluster.) . You can later take down the service and its deployment by running: . kubectl delete -f model_serve.yaml . Send prediction requests to your deployed model service . Once your model serving service is deployed, you can send prediction requests to it. Because we didn’t set up an external endpoint for our service in this simple example, we’ll connect to the service via port forwarding. From the command line, run the following, replacing &lt;your-model-name&gt; with the value you replaced MODEL_NAME by, when creating your yaml file, and &lt;service-namespace&gt; with the namespace in which your service is running— the same namespace value you used in the yaml file. . kubectl -n &lt;service-namespace&gt; port-forward svc/&lt;your-model-name&gt; 8080:80 . Then, from the deploy_model_for_tables directory, send a prediction request to your service like this: . curl -X POST --data @./instances.json http://localhost:8080/predict . You should see a result like this, with a prediction for each instance in the instances.json file: . {&quot;predictions&quot;: [860.79833984375, 460.5323486328125, 1211.7664794921875]} . (If you get an error, make sure you’re in the correct directory and see the instances.json file listed). . Note: it would be possible to add this deployment step to the pipeline too. (See deploy_model_for_tables/exported_model_deploy.py). However, the Python client library does not yet support the ‘export’ operation. Once deployment is supported by the client library, this would be a natural addition to the workflow. While not tested, it should also be possible to do the export programmatically via the REST API. . A deeper dive into the pipeline code . The updated Tables Python client library makes it very straightforward to build the Pipelines components that support each stage of the workflow. Kubeflow Pipeline steps are container-based, so that any action you can support via a Docker container image can become a pipeline step. That doesn’t mean that an end-user necessarily needs to have Docker installed. For many straightforward cases, building your pipeline steps . Using the ‘lightweight python components’ functionality to build pipeline steps . For most of the components in this example, we’re building them using the “lightweight python components” functionality as shown in this example notebook, including compilation of the code into a component package. This feature allows you to create components based on Python functions, building on an appropriate base image, so that you do not need to have docker installed or rebuild a container image each time your code changes. . Each component’s python file includes a function definition, and then a func_to_container_op call, passing the function definition, to generate the component’s yaml package file. As we’ll see below, these component package files make it very straightforward to put these steps together to form a pipeline. . The deploy_model_for_tables/tables_deploy_component.py file is representative. It contains an automl_deploy_tables_model function definition. . def automl_deploy_tables_model( gcp_project_id: str, gcp_region: str, model_display_name: str, api_endpoint: str = None, ) -&gt; NamedTuple(&#39;Outputs&#39;, [(&#39;model_display_name&#39;, str), (&#39;status&#39;, str)]): ... return (model_display_name, status) . The function defines the component’s inputs and outputs, and this information will be used to support static checking when we compose these components to build the pipeline. . To build the component yaml file corresponding to this function, we add the following to the components’ Python script, then can run python &lt;filename&gt;.py from the command line to generate it (you must have the Kubeflow Pipelines (KFP) sdk installed). . if __name__ == &#39;__main__&#39;: import kfp kfp.components.func_to_container_op( automl_deploy_tables_model, output_component_file=&#39;tables_deploy_component.yaml&#39;, base_image=&#39;python:3.7&#39;) . Whenever you change the python function definition, just recompile to regenerate the corresponding component file. . Specifying the Tables pipeline . With the components packaged into yaml files, it becomes very straightforward to specify a pipeline, such as tables_pipeline_caip.py, that uses them. Here, we’re just using the load_component_from_file() method, since the yaml files are all local (in the same repo). However, there is also a load_component_from_url() method, which makes it easy to share components. (If your URL points to a file in GitHub, be sure to use raw mode). . create_dataset_op = comp.load_component_from_file( &#39;./create_dataset_for_tables/tables_component.yaml&#39;) import_data_op = comp.load_component_from_file( &#39;./import_data_from_bigquery/tables_component.yaml&#39;) set_schema_op = comp.load_component_from_file( &#39;./import_data_from_bigquery/tables_schema_component.yaml&#39;) train_model_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_component.yaml&#39;) eval_model_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_eval_component.yaml&#39;) eval_metrics_op = comp.load_component_from_file( &#39;./create_model_for_tables/tables_eval_metrics_component.yaml&#39;) deploy_model_op = comp.load_component_from_file( &#39;./deploy_model_for_tables/tables_deploy_component.yaml&#39;) . Once all our pipeline ops (steps) are defined using the component definitions, then we can specify the pipeline by calling the constructors, e.g.: . create_dataset = create_dataset_op( gcp_project_id=gcp_project_id, gcp_region=gcp_region, dataset_display_name=dataset_display_name, api_endpoint=api_endpoint, ) . If a pipeline component has been defined to have outputs, other components can access those outputs. E.g., here, the ‘eval’ step is grabbing an output from the ‘train’ step, specifically information about the model display name: . eval_model = eval_model_op( gcp_project_id=gcp_project_id, gcp_region=gcp_region, bucket_name=bucket_name, gcs_path=&#39;automl_evals/{}&#39;.format(dsl.RUN_ID_PLACEHOLDER), api_endpoint=api_endpoint, model_display_name=train_model.outputs[&#39;model_display_name&#39;] ) . In this manner it is straightforward to put together a pipeline from your component definitions. Just don’t forget to recompile the pipeline script (to generate its corresponding .tar.gz archive) if any of its component definitions changed, e.g. python tables_pipeline_caip.py. .",
            "url": "https://amygdala.github.io/gcp_blog/ml/kfp/automl/2020/04/22/automltables_kfp_e2e.html",
            "relUrl": "/ml/kfp/automl/2020/04/22/automltables_kfp_e2e.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Getting explanations for AutoML Tables predictions",
            "content": "Introduction . Google Cloud’s AutoML Tables lets you automatically build and deploy state-of-the-art machine learning models using your own structured data. . AutoML Tables now has an easier-to-use Tables-specific Python client library, as well as a new ability to explain online prediction results— called local feature importance— which gives visibility into how the features in a specific prediction request informed the resulting prediction. You can read more about explainable AI for Tables in this blog post. . The source for this post is a Jupyter notebook. In this notebook, we&#39;ll create a custom Tables model to predict duration of London bike rentals given information about local weather as well as info about the rental trip. We&#39;ll walk through examples of using the Tables client libraries for creating a dataset, training a custom model, deploying the model, and using it to make predictions; and show how you can programmatically request local feature importance information. . We recommend running this notebook using AI Platform Notebooks. If you want to run the notebook on colab (or locally), it&#39;s possible, but you&#39;ll need to do a bit more setup. See the Appendix section of this notebook for details. . Before you begin . Follow the AutoML Tables documentation to: . Select or create a GCP project. | Make sure that billing is enabled for your project | Enable the Cloud AutoML and Storage APIs. | (Recommended) Create an AI Platform Notebook instance and upload this notebook to it. | . (See also the Quickstart guide for a getting-started walkthrough on AutoML Tables). . Then, install the AutoML Python client libraries into your notebook environment: . !pip3 install -U google-cloud-automl . You may need to restart your notebook kernel after running the above to pick up the installation. . Enter your GCP project ID in the cell below, then run the cell. . PROJECT_ID = &quot;&lt;your-project-id&gt;&quot; . Do some imports . Next, import some libraries and set some variables. . import argparse import os from google.api_core.client_options import ClientOptions from google.cloud import automl_v1beta1 as automl import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types . REGION = &#39;us-central1&#39; DATASET_NAME = &#39;bikes-weather&#39; BIGQUERY_PROJECT_ID = &#39;aju-dev-demos&#39; DATASET_ID = &#39;london_bikes_weather&#39; TABLE_ID = &#39;bikes_weather&#39; IMPORT_URI = &#39;bq://%s.%s.%s&#39; % (BIGQUERY_PROJECT_ID, DATASET_ID, TABLE_ID) print(IMPORT_URI) . DATASET_NAME = &#39;bikes_weather&#39; . Create a dataset, and import data . Next, we&#39;ll define some utility functions to create a dataset, and to import data into a dataset. The client.import_data() call returns an operation future that can be used to check for completion synchronously or asynchronously— in this case we wait synchronously. . def create_dataset(client, dataset_display_name): &quot;&quot;&quot;Create a dataset.&quot;&quot;&quot; # Create a dataset with the given display name dataset = client.create_dataset(dataset_display_name) # Display the dataset information. print(&quot;Dataset name: {}&quot;.format(dataset.name)) print(&quot;Dataset id: {}&quot;.format(dataset.name.split(&quot;/&quot;)[-1])) print(&quot;Dataset display name: {}&quot;.format(dataset.display_name)) print(&quot;Dataset metadata:&quot;) print(&quot; t{}&quot;.format(dataset.tables_dataset_metadata)) print(&quot;Dataset example count: {}&quot;.format(dataset.example_count)) print(&quot;Dataset create time:&quot;) print(&quot; tseconds: {}&quot;.format(dataset.create_time.seconds)) print(&quot; tnanos: {}&quot;.format(dataset.create_time.nanos)) return dataset . def import_data(client, dataset_display_name, path): &quot;&quot;&quot;Import structured data.&quot;&quot;&quot; response = None if path.startswith(&#39;bq&#39;): response = client.import_data( dataset_display_name=dataset_display_name, bigquery_input_uri=path ) else: # Get the multiple Google Cloud Storage URIs. input_uris = path.split(&quot;,&quot;) response = client.import_data( dataset_display_name=dataset_display_name, gcs_input_uris=input_uris ) print(&quot;Processing import...&quot;) # synchronous check of operation status. print(&quot;Data imported. {}&quot;.format(response.result())) . Next, we&#39;ll create the client object that we&#39;ll use for all our operations. . client = automl.TablesClient(project=PROJECT_ID, region=REGION) . Create the Tables dataset: . create_dataset(client, DATASET_NAME) . ... and then import data from the BigQuery table into the dataset. The import command will take a while to run. Wait until it has returned before proceeding. You can also check import status in the Cloud Console. . (Note that if you run this notebook multiple times, you will get an error if you try to create multiple datasets with the same name. However, you can train multiple models against the same dataset.) . import_data(client, DATASET_NAME, IMPORT_URI) . Update the dataset schema . Now we&#39;ll define utility functions to update dataset and column information. We need these to set the dataset&#39;s target column (the field we&#39;ll train our model to predict) and to change the types of some of the columns. AutoML Tables is pretty good at inferring reasonable column types based on input, but in our case, there are some columns (like bike station IDs) that we want to treat as Categorical instead of Numeric. . def update_column_spec(client, dataset_display_name, column_spec_display_name, type_code, nullable=None): &quot;&quot;&quot;Update column spec.&quot;&quot;&quot; response = client.update_column_spec( dataset_display_name=dataset_display_name, column_spec_display_name=column_spec_display_name, type_code=type_code, nullable=nullable ) # synchronous check of operation status. print(&quot;Table spec updated. {}&quot;.format(response)) def update_dataset(client, dataset_display_name, target_column_spec_name=None, time_column_spec_name=None, test_train_column_spec_name=None): &quot;&quot;&quot;Update dataset.&quot;&quot;&quot; if target_column_spec_name is not None: response = client.set_target_column( dataset_display_name=dataset_display_name, column_spec_display_name=target_column_spec_name ) print(&quot;Target column updated. {}&quot;.format(response)) if time_column_spec_name is not None: response = client.set_time_column( dataset_display_name=dataset_display_name, column_spec_display_name=time_column_spec_name ) print(&quot;Time column updated. {}&quot;.format(response)) . def list_column_specs(client, dataset_display_name, filter_=None): &quot;&quot;&quot;List all column specs.&quot;&quot;&quot; result = [] # List all the table specs in the dataset by applying filter. response = client.list_column_specs( dataset_display_name=dataset_display_name, filter_=filter_) print(&quot;List of column specs:&quot;) for column_spec in response: # Display the column_spec information. print(&quot;Column spec name: {}&quot;.format(column_spec.name)) print(&quot;Column spec id: {}&quot;.format(column_spec.name.split(&quot;/&quot;)[-1])) print(&quot;Column spec display name: {}&quot;.format(column_spec.display_name)) print(&quot;Column spec data type: {}&quot;.format(column_spec.data_type)) result.append(column_spec) return result . Update the dataset to indicate that the target column is duration. . update_dataset(client, DATASET_NAME, target_column_spec_name=&#39;duration&#39;, # time_column_spec_name=&#39;ts&#39; ) . Now we&#39;ll update some of the column types. You can list their default specs first if you like: . list_column_specs(client, DATASET_NAME) . ... and now we&#39;ll update them to the types we want: . update_column_spec(client, DATASET_NAME, &#39;end_station_id&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;start_station_id&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;loc_cross&#39;, &#39;CATEGORY&#39;) update_column_spec(client, DATASET_NAME, &#39;bike_id&#39;, &#39;CATEGORY&#39;) . You can view the results in the Cloud Console. Note that useful stats are generated for each column. You can also run the list_column_specs() function again to see the new config. . # list_column_specs(client, DATASET_NAME) . Train a custom model on the dataset . Now we&#39;re ready to train a model on the dataset. We&#39;ll need to generate a unique name for the model, which we&#39;ll do by appending a timestamp, in case you want to run this notebook multiple times. The 1000 arg in the create_model() call specifies to budget 1 hour of training time. . In the create_model() utility function below, we may not want to block on the result, since total job time can be multiple hours. If you want the function to block until training is complete, uncomment the last line of the function below. . import time MODEL_NAME = &#39;bwmodel_&#39; + str(int(time.time())) print(&#39;MODEL_NAME: %s&#39; % MODEL_NAME) def create_model(client, dataset_display_name, model_display_name, train_budget_milli_node_hours, include_column_spec_names=None, exclude_column_spec_names=None): &quot;&quot;&quot;Create a model.&quot;&quot;&quot; # Create a model with the model metadata in the region. response = client.create_model( model_display_name, train_budget_milli_node_hours=train_budget_milli_node_hours, dataset_display_name=dataset_display_name, include_column_spec_names=include_column_spec_names, exclude_column_spec_names=exclude_column_spec_names, ) print(&quot;Training model...&quot;) print(&quot;Training operation: {}&quot;.format(response.operation)) print(&quot;Training operation name: {}&quot;.format(response.operation.name)) # uncomment the following to block until training is finished. # print(&quot;Training completed: {}&quot;.format(response.result())) . create_model(client, DATASET_NAME, MODEL_NAME, 1000) . Get the status of your training job . Edit the following call to set OP_NAME to the &quot;training operation name&quot; listed in the output of create_model() above. . OP_NAME = &#39;YOUR TRAINING OPERATION NAME&#39; . def get_operation_status(client, operation_full_id): &quot;&quot;&quot;Get operation status.&quot;&quot;&quot; # Get the latest state of a long-running operation. op = client.auto_ml_client.transport._operations_client.get_operation( operation_full_id ) print(&quot;Operation status: {}&quot;.format(op)) from google.cloud.automl import types msg = types.OperationMetadata() print(msg.ParseFromString(op.metadata.value)) . The training job may take several hours. You can check on its status in the Cloud Console UI. You can also monitor it via the get_operation_status() call below. (Make sure you&#39;ve edited the OP_NAME variable value above). You&#39;ll see: done: true in the output when it&#39;s finished. . (Note: if you should lose your notebook kernel context while the training job is running, you can continue the rest of the notebook later with a new kernel: just make note of the MODEL_NAME. You can find that information in the Cloud Console as well). . res = get_operation_status(client, OP_NAME) . Get information about your trained custom model . Once it has been created, you can get information about a specific model. (While the training job is still running, you&#39;ll just get a not found message.) . from google.cloud.automl_v1beta1 import enums from google.api_core import exceptions def get_model(client, model_display_name): &quot;&quot;&quot;Get model details.&quot;&quot;&quot; try: model = client.get_model(model_display_name=model_display_name) except exceptions.NotFound: print(&quot;Model %s not found.&quot; % model_display_name) return (None, None) # Get complete detail of the model.a model = client.get_model(model_display_name=model_display_name) # Retrieve deployment state. if model.deployment_state == enums.Model.DeploymentState.DEPLOYED: deployment_state = &quot;deployed&quot; else: deployment_state = &quot;undeployed&quot; # get features of top global importance feat_list = [ (column.feature_importance, column.column_display_name) for column in model.tables_model_metadata.tables_model_column_info ] feat_list.sort(reverse=True) if len(feat_list) &lt; 10: feat_to_show = len(feat_list) else: feat_to_show = 10 # Display the model information. print(&quot;Model name: {}&quot;.format(model.name)) print(&quot;Model id: {}&quot;.format(model.name.split(&quot;/&quot;)[-1])) print(&quot;Model display name: {}&quot;.format(model.display_name)) print(&quot;Features of top importance:&quot;) for feat in feat_list[:feat_to_show]: print(feat) print(&quot;Model create time:&quot;) print(&quot; tseconds: {}&quot;.format(model.create_time.seconds)) print(&quot; tnanos: {}&quot;.format(model.create_time.nanos)) print(&quot;Model deployment state: {}&quot;.format(deployment_state)) return (model, feat_list) . Don&#39;t proceed with the rest of the notebook until the model has finished training and the following get_model() call returns model information rather than &#39;not found&#39;. . Once the training job has finished, we can get information about the model, including information about which input features proved to be the most important globally (that is, across the full training dataset). . (model, global_feat_importance) = get_model(client, MODEL_NAME) . We can graph the global feature importance values to get a visualization of which inputs were most important in training the model. (The Cloud Console UI also displays such a graph). . print(global_feat_importance) . import matplotlib.pyplot as plt res = list(zip(*global_feat_importance)) x = list(res[0]) y = list(res[1]) y_pos = list(range(len(y))) plt.barh(y_pos, x, alpha=0.5) plt.yticks(y_pos, y) plt.show() . See your model&#39;s evaluation metrics . We can also get model evaluation information once the model is trained. The available metrics depend upon which optimization objective you used. In this example, we used the default, RMSE. . evals = client.list_model_evaluations(model_display_name=MODEL_NAME) list(evals)[1].regression_evaluation_metrics . Use your trained model to make predictions and see explanations of the results . Deploy your model and get predictions + explanations . Once your training job has finished, you can use your model to make predictions. . With online prediction, you can now request explanations of the results, in the form of local feature importance calculations on the inputs. Local feature importance gives you visibility into how the features in a specific prediction request informed the resulting prediction. . To get online predictions, we first need to deploy the model. . Note: see the documentation for other prediction options including the ability to export your custom model and run it in a container anywhere. . def deploy_model(client, model_display_name): &quot;&quot;&quot;Deploy model.&quot;&quot;&quot; response = client.deploy_model(model_display_name=model_display_name) # synchronous check of operation status. print(&quot;Model deployed. {}&quot;.format(response.result())) . It will take a while to deploy the model. Wait for the deploy_model() call to finish before proceeding with the rest of the notebook cells. You can track status in the Console UI as well. . deploy_model(client, MODEL_NAME) . Once the model is deployed, you can access it via the UI, or the API, to make online prediction requests. These can include a request for local feature importance calculations on the inputs, a newly-launched feature. Local feature importance gives you visibility into how the features in a specific prediction request informed the resulting prediction. . def predict(client, model_display_name, inputs, feature_importance=False): &quot;&quot;&quot;Make a prediction.&quot;&quot;&quot; if feature_importance: response = client.predict( model_display_name=model_display_name, inputs=inputs, feature_importance=True, ) else: response = client.predict( model_display_name=model_display_name, inputs=inputs) print(&quot;Prediction results:&quot;) print(response) return response . inputs = { &quot;bike_id&quot;: &quot;5373&quot;, &quot;day_of_week&quot;: &quot;3&quot;, &quot;end_latitude&quot;: 51.52059681, &quot;end_longitude&quot;: -0.116688468, &quot;end_station_id&quot;: &quot;68&quot;, &quot;euclidean&quot;: 3589.5146210024977, &quot;loc_cross&quot;: &quot;POINT(-0.07 51.52)POINT(-0.12 51.52)&quot;, &quot;max&quot;: 44.6, &quot;min&quot;: 34.0, &quot;prcp&quot;: 0, &quot;ts&quot;: &quot;1480407420&quot;, &quot;start_latitude&quot;: 51.52388, &quot;start_longitude&quot;: -0.065076, &quot;start_station_id&quot;: &quot;445&quot;, &quot;temp&quot;: 38.2, &quot;dewp&quot;: 28.6 } . Try running the prediction request first without, then with, the local feature importance calculations, to see the difference in the information that is returned. (The actual duration— that we&#39;re predicting— is 1200.) . predict(client, MODEL_NAME, inputs, feature_importance=False) . response = predict(client, MODEL_NAME, inputs, feature_importance=True) . We can plot the local feature importance values to get a visualization of which fields were most and least important for this particular prediction. . import matplotlib.pyplot as plt col_info = response.payload[0].tables.tables_model_column_info x = [] y = [] for c in col_info: y.append(c.column_display_name) x.append(c.feature_importance) y_pos = list(range(len(y))) plt.barh(y_pos, x, alpha=0.5) plt.yticks(y_pos, y) plt.show() . You can see a similar graphic in the Cloud Console Tables UI when you submit an ONLINE PREDICTION and tick the &quot;Generate feature importance&quot; checkbox. . The local feature importance calculations are specific to a given input instance. . Summary . In this notebook, we showed how you can use the AutoML Tables client library to create datasets, train models, and get predictions from your trained model— and in particular, how you can get explanations of the results along with the predictions. . Appendix: running this notebook on colab (or locally) . It&#39;s possible to run this example on colab, but it takes a bit more setup. Do the following before you create the Tables client object or call the API. . Create a service account, give it the necessary roles (e.g., AutoML Admin) and download a json credentials file for the service account. Upload the credentials file to the colab file system. . Then, edit the following to point to that file, and run the cell: . %env GOOGLE_APPLICATION_CREDENTIALS /content/your-credentials-file.json . Your Tables API calls should now be properly authenticated. If you lose the colab runtime, you&#39;ll need to re-upload the file and re-set the environment variable. . If you&#39;re running the notebook locally, point the GOOGLE_APPLICATION_CREDENTIALS environment variable to the service account credentials file before starting the notebook, e.g.: . export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your-credentials-file.json .",
            "url": "https://amygdala.github.io/gcp_blog/ml/xai/automl/jupyter/2020/04/17/automl_tables_xai.html",
            "relUrl": "/ml/xai/automl/jupyter/2020/04/17/automl_tables_xai.html",
            "date": " • Apr 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I work with the Google Cloud Platform, with a focus on Machine Learning. .",
          "url": "https://amygdala.github.io/gcp_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amygdala.github.io/gcp_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}